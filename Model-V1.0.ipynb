{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Select Jupyter Server --> Existing --> http://192.168.8.10:8888/\n",
    "## Password int2\n",
    "\n",
    "\n",
    "# Example Material Source: https://www.tensorflow.org/tutorials/images/cnn\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models, Sequential, regularizers,optimizers,callbacks\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 1.]\n [0. 0. 0. ... 0. 0. 1.]\n ...\n [0. 0. 0. ... 0. 0. 1.]\n [0. 1. 0. ... 0. 0. 0.]\n [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 32, 32, 3)\n",
    "test_images = test_images.reshape(test_images.shape[0], 32, 32, 3)\n",
    "\n",
    "\n",
    "##one hot encoding\n",
    "train_labels = np_utils.to_categorical(train_labels,10)\n",
    "test_labels = np_utils.to_categorical(test_labels,10)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "##train_images, test_images = preProcess(train_images), preProcess(test_images)\n",
    "##train_images, test_images = contrastCurve(train_images), contrastCurve(test_images)\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming images to get a larger test set\n",
    "\n",
    "data_augmentation = Sequential(\n",
    "  [\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\",\n",
    "    input_shape=(64,64,3)),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.15),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.05),\n",
    "    ##layers.experimental.preprocessing.RandomContrast(0.075),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##tf.keras.layers.Conv2D(\n",
    "##    filters, kernel_size, strides=(1, 1), padding='valid',\n",
    "##    data_format=None, dilation_rate=(1, 1), groups=1, activation=None,\n",
    "##    use_bias=True, kernel_initializer='glorot_uniform',\n",
    "##    bias_initializer='zeros', kernel_regularizer=None,\n",
    "##    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "##    bias_constraint=None, **kwargs\n",
    "##)\n",
    "\n",
    "##--------------  VGG Based + Dynamic Learning Rates --------------\n",
    "\n",
    "def learing_rate_callback(epoch_number,lr):\n",
    "    if(epoch_number < 50):\n",
    "        return 0.005\n",
    "    elif(epoch_number >50 and epoch_number <100):\n",
    "        return 0.0005\n",
    "\n",
    "    return lr*0.995\n",
    "    \n",
    "\n",
    "model = models.Sequential()\n",
    "## Up sampling to try and get more feature points and allow for more layers\n",
    "model.add(layers.UpSampling2D(size=(2,2),interpolation=\"bilinear\",input_shape=(32, 32, 3)))\n",
    "model.add(data_augmentation)\n",
    "\n",
    "weight_decay = 0.00011181\n",
    "weight_decay = 8.950000000000001e-05\n",
    "dropout_rate = 0.3\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(64, 64, 3),kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(dropout_rate*0.75))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Dropout(dropout_rate*0.90))\n",
    "\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Dropout(dropout_rate))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Dropout(dropout_rate*0.90))\n",
    "\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Dropout(dropout_rate))\n",
    "model.add(layers.Activation('elu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(256, (1, 1), activation='relu', kernel_initializer='he_uniform', padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "\n",
    "\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu', kernel_initializer='he_uniform',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(layers.Dropout(dropout_rate))\n",
    "##model.add(layers.Dense(512, activation='relu', kernel_initializer='he_uniform',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "##model.add(layers.Dropout(dropout_rate))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "# compile model\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nup_sampling2d_1 (UpSampling2 (None, 64, 64, 3)         0         \n_________________________________________________________________\nsequential_2 (Sequential)    (None, 64, 64, 3)         0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 64, 64, 32)        896       \n_________________________________________________________________\nactivation_10 (Activation)   (None, 64, 64, 32)        0         \n_________________________________________________________________\nbatch_normalization_10 (Batc (None, 64, 64, 32)        128       \n_________________________________________________________________\nconv2d_11 (Conv2D)           (None, 64, 64, 32)        9248      \n_________________________________________________________________\nactivation_11 (Activation)   (None, 64, 64, 32)        0         \n_________________________________________________________________\nbatch_normalization_11 (Batc (None, 64, 64, 32)        128       \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 32, 32, 32)        0         \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 32, 32, 32)        0         \n_________________________________________________________________\nconv2d_12 (Conv2D)           (None, 32, 32, 64)        18496     \n_________________________________________________________________\nactivation_12 (Activation)   (None, 32, 32, 64)        0         \n_________________________________________________________________\nbatch_normalization_12 (Batc (None, 32, 32, 64)        256       \n_________________________________________________________________\nconv2d_13 (Conv2D)           (None, 32, 32, 64)        36928     \n_________________________________________________________________\nactivation_13 (Activation)   (None, 32, 32, 64)        0         \n_________________________________________________________________\nbatch_normalization_13 (Batc (None, 32, 32, 64)        256       \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 16, 16, 64)        0         \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 16, 16, 64)        0         \n_________________________________________________________________\nactivation_14 (Activation)   (None, 16, 16, 64)        0         \n_________________________________________________________________\nbatch_normalization_14 (Batc (None, 16, 16, 64)        256       \n_________________________________________________________________\nconv2d_14 (Conv2D)           (None, 16, 16, 128)       73856     \n_________________________________________________________________\nactivation_15 (Activation)   (None, 16, 16, 128)       0         \n_________________________________________________________________\nbatch_normalization_15 (Batc (None, 16, 16, 128)       512       \n_________________________________________________________________\nconv2d_15 (Conv2D)           (None, 16, 16, 128)       147584    \n_________________________________________________________________\nactivation_16 (Activation)   (None, 16, 16, 128)       0         \n_________________________________________________________________\nbatch_normalization_16 (Batc (None, 16, 16, 128)       512       \n_________________________________________________________________\nconv2d_16 (Conv2D)           (None, 16, 16, 128)       147584    \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 16, 16, 128)       0         \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 8, 8, 128)         0         \n_________________________________________________________________\nmax_pooling2d_8 (MaxPooling2 (None, 4, 4, 128)         0         \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 4, 4, 128)         0         \n_________________________________________________________________\nactivation_17 (Activation)   (None, 4, 4, 128)         0         \n_________________________________________________________________\nbatch_normalization_17 (Batc (None, 4, 4, 128)         512       \n_________________________________________________________________\nconv2d_17 (Conv2D)           (None, 4, 4, 256)         295168    \n_________________________________________________________________\nactivation_18 (Activation)   (None, 4, 4, 256)         0         \n_________________________________________________________________\nbatch_normalization_18 (Batc (None, 4, 4, 256)         1024      \n_________________________________________________________________\nconv2d_18 (Conv2D)           (None, 4, 4, 256)         590080    \n_________________________________________________________________\nactivation_19 (Activation)   (None, 4, 4, 256)         0         \n_________________________________________________________________\nbatch_normalization_19 (Batc (None, 4, 4, 256)         1024      \n_________________________________________________________________\nconv2d_19 (Conv2D)           (None, 4, 4, 256)         590080    \n_________________________________________________________________\ndropout_11 (Dropout)         (None, 4, 4, 256)         0         \n_________________________________________________________________\nmax_pooling2d_9 (MaxPooling2 (None, 2, 2, 256)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 1024)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 512)               524800    \n_________________________________________________________________\ndropout_12 (Dropout)         (None, 512)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 10)                5130      \n=================================================================\nTotal params: 2,444,458\nTrainable params: 2,442,154\nNon-trainable params: 2,304\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "al_accuracy: 0.8988\n",
      "Epoch 166/300\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.3486 - accuracy: 0.9279 - val_loss: 0.4416 - val_accuracy: 0.8986\n",
      "Epoch 167/300\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.3489 - accuracy: 0.9259 - val_loss: 0.4166 - val_accuracy: 0.9056\n",
      "Epoch 168/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3450 - accuracy: 0.9276 - val_loss: 0.4264 - val_accuracy: 0.9064\n",
      "Epoch 169/300\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.3426 - accuracy: 0.9289 - val_loss: 0.4278 - val_accuracy: 0.9063\n",
      "Epoch 170/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3458 - accuracy: 0.9268 - val_loss: 0.4341 - val_accuracy: 0.9015\n",
      "Epoch 171/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3489 - accuracy: 0.9268 - val_loss: 0.4322 - val_accuracy: 0.9035\n",
      "Epoch 172/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3440 - accuracy: 0.9290 - val_loss: 0.4217 - val_accuracy: 0.9060\n",
      "Epoch 173/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3390 - accuracy: 0.9311 - val_loss: 0.4451 - val_accuracy: 0.8971\n",
      "Epoch 174/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3420 - accuracy: 0.9279 - val_loss: 0.4429 - val_accuracy: 0.8990\n",
      "Epoch 175/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3365 - accuracy: 0.9306 - val_loss: 0.4323 - val_accuracy: 0.9025\n",
      "Epoch 176/300\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.3338 - accuracy: 0.9305 - val_loss: 0.4195 - val_accuracy: 0.9067\n",
      "Epoch 177/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3395 - accuracy: 0.9286 - val_loss: 0.4317 - val_accuracy: 0.9028\n",
      "Epoch 178/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3372 - accuracy: 0.9291 - val_loss: 0.4384 - val_accuracy: 0.8981\n",
      "Epoch 179/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3357 - accuracy: 0.9302 - val_loss: 0.4350 - val_accuracy: 0.8998\n",
      "Epoch 180/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3387 - accuracy: 0.9281 - val_loss: 0.4170 - val_accuracy: 0.9055\n",
      "Epoch 181/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3301 - accuracy: 0.9331 - val_loss: 0.4138 - val_accuracy: 0.9070\n",
      "Epoch 182/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3325 - accuracy: 0.9296 - val_loss: 0.4191 - val_accuracy: 0.9013\n",
      "Epoch 183/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3308 - accuracy: 0.9315 - val_loss: 0.4438 - val_accuracy: 0.8957\n",
      "Epoch 184/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3285 - accuracy: 0.9305 - val_loss: 0.4211 - val_accuracy: 0.9035\n",
      "Epoch 185/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3285 - accuracy: 0.9318 - val_loss: 0.4065 - val_accuracy: 0.9098\n",
      "Epoch 186/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3316 - accuracy: 0.9312 - val_loss: 0.4147 - val_accuracy: 0.9054\n",
      "Epoch 187/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3251 - accuracy: 0.9324 - val_loss: 0.4178 - val_accuracy: 0.9076\n",
      "Epoch 188/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3301 - accuracy: 0.9312 - val_loss: 0.4196 - val_accuracy: 0.9039\n",
      "Epoch 189/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3282 - accuracy: 0.9311 - val_loss: 0.4269 - val_accuracy: 0.9015\n",
      "Epoch 190/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3209 - accuracy: 0.9340 - val_loss: 0.4341 - val_accuracy: 0.9009\n",
      "Epoch 191/300\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.3224 - accuracy: 0.9333 - val_loss: 0.4236 - val_accuracy: 0.9044\n",
      "Epoch 192/300\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.3217 - accuracy: 0.9330 - val_loss: 0.4233 - val_accuracy: 0.9043\n",
      "Epoch 193/300\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.3234 - accuracy: 0.9328 - val_loss: 0.4211 - val_accuracy: 0.9013\n",
      "Epoch 194/300\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.3227 - accuracy: 0.9347 - val_loss: 0.4344 - val_accuracy: 0.8993\n",
      "Epoch 195/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3175 - accuracy: 0.9354 - val_loss: 0.4193 - val_accuracy: 0.9028\n",
      "Epoch 196/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3216 - accuracy: 0.9331 - val_loss: 0.4194 - val_accuracy: 0.9025\n",
      "Epoch 197/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3187 - accuracy: 0.9322 - val_loss: 0.4123 - val_accuracy: 0.9057\n",
      "Epoch 198/300\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.3138 - accuracy: 0.9355 - val_loss: 0.4108 - val_accuracy: 0.9075\n",
      "Epoch 199/300\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.3161 - accuracy: 0.9348 - val_loss: 0.4385 - val_accuracy: 0.8967\n",
      "Epoch 200/300\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.3144 - accuracy: 0.9347 - val_loss: 0.4164 - val_accuracy: 0.9067\n",
      "Epoch 201/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3055 - accuracy: 0.9381 - val_loss: 0.4093 - val_accuracy: 0.9066\n",
      "Epoch 202/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3115 - accuracy: 0.9357 - val_loss: 0.4189 - val_accuracy: 0.9062\n",
      "Epoch 203/300\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.3139 - accuracy: 0.9354 - val_loss: 0.4205 - val_accuracy: 0.9021\n",
      "Epoch 204/300\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.3041 - accuracy: 0.9384 - val_loss: 0.4366 - val_accuracy: 0.9004\n",
      "Epoch 205/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3116 - accuracy: 0.9347 - val_loss: 0.4025 - val_accuracy: 0.9091\n",
      "Epoch 206/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3099 - accuracy: 0.9344 - val_loss: 0.4077 - val_accuracy: 0.9083\n",
      "Epoch 207/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3086 - accuracy: 0.9372 - val_loss: 0.4135 - val_accuracy: 0.9062\n",
      "Epoch 208/300\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.3131 - accuracy: 0.9346 - val_loss: 0.4246 - val_accuracy: 0.9038\n",
      "Epoch 209/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3075 - accuracy: 0.9364 - val_loss: 0.4245 - val_accuracy: 0.9015\n",
      "Epoch 210/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3120 - accuracy: 0.9339 - val_loss: 0.4102 - val_accuracy: 0.9046\n",
      "Epoch 211/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3028 - accuracy: 0.9365 - val_loss: 0.4183 - val_accuracy: 0.9041\n",
      "Epoch 212/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3070 - accuracy: 0.9378 - val_loss: 0.4170 - val_accuracy: 0.9071\n",
      "Epoch 213/300\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.3020 - accuracy: 0.9384 - val_loss: 0.4314 - val_accuracy: 0.8984\n",
      "Epoch 214/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3076 - accuracy: 0.9372 - val_loss: 0.4111 - val_accuracy: 0.9069\n",
      "Epoch 215/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3110 - accuracy: 0.9358 - val_loss: 0.4141 - val_accuracy: 0.9063\n",
      "Epoch 216/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3116 - accuracy: 0.9341 - val_loss: 0.4075 - val_accuracy: 0.9095\n",
      "Epoch 217/300\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.2988 - accuracy: 0.9384 - val_loss: 0.4072 - val_accuracy: 0.9067\n",
      "Epoch 218/300\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.3092 - accuracy: 0.9337 - val_loss: 0.4166 - val_accuracy: 0.9041\n",
      "Epoch 219/300\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.3001 - accuracy: 0.9377 - val_loss: 0.4200 - val_accuracy: 0.9050\n",
      "Epoch 220/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3054 - accuracy: 0.9361 - val_loss: 0.4040 - val_accuracy: 0.9067\n",
      "Epoch 221/300\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.3001 - accuracy: 0.9384 - val_loss: 0.4036 - val_accuracy: 0.9090\n",
      "Epoch 222/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.2966 - accuracy: 0.9379 - val_loss: 0.4156 - val_accuracy: 0.9059\n",
      "Epoch 223/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.3014 - accuracy: 0.9360 - val_loss: 0.4120 - val_accuracy: 0.9066\n",
      "Epoch 224/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.2990 - accuracy: 0.9403 - val_loss: 0.4133 - val_accuracy: 0.9068\n",
      "Epoch 225/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.2949 - accuracy: 0.9395 - val_loss: 0.4127 - val_accuracy: 0.9029\n",
      "Epoch 226/300\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.2908 - accuracy: 0.9413 - val_loss: 0.4034 - val_accuracy: 0.9097\n",
      "Epoch 227/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.2974 - accuracy: 0.9375 - val_loss: 0.4157 - val_accuracy: 0.9076\n",
      "Epoch 228/300\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.2945 - accuracy: 0.9413 - val_loss: 0.4104 - val_accuracy: 0.9081\n",
      "Epoch 229/300\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.2889 - accuracy: 0.9426 - val_loss: 0.4114 - val_accuracy: 0.9037\n",
      "Epoch 230/300\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.2931 - accuracy: 0.9404 - val_loss: 0.4110 - val_accuracy: 0.9045\n",
      "Epoch 231/300\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.2855 - accuracy: 0.9427 - val_loss: 0.4019 - val_accuracy: 0.9072\n",
      "Epoch 232/300\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.2896 - accuracy: 0.9399 - val_loss: 0.4150 - val_accuracy: 0.9057\n",
      "Epoch 233/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2905 - accuracy: 0.9405 - val_loss: 0.3997 - val_accuracy: 0.9064\n",
      "Epoch 234/300\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.2895 - accuracy: 0.9399 - val_loss: 0.4129 - val_accuracy: 0.9041\n",
      "Epoch 235/300\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.2861 - accuracy: 0.9405 - val_loss: 0.4048 - val_accuracy: 0.9079\n",
      "Epoch 236/300\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.2865 - accuracy: 0.9400 - val_loss: 0.4094 - val_accuracy: 0.9067\n",
      "Epoch 237/300\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.2819 - accuracy: 0.9418 - val_loss: 0.4021 - val_accuracy: 0.9073\n",
      "Epoch 238/300\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.2828 - accuracy: 0.9427 - val_loss: 0.4041 - val_accuracy: 0.9067\n",
      "Epoch 239/300\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.2849 - accuracy: 0.9411 - val_loss: 0.4059 - val_accuracy: 0.9079\n",
      "Epoch 240/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2834 - accuracy: 0.9424 - val_loss: 0.4051 - val_accuracy: 0.9058\n",
      "Epoch 241/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2838 - accuracy: 0.9419 - val_loss: 0.4129 - val_accuracy: 0.9023\n",
      "Epoch 242/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2867 - accuracy: 0.9419 - val_loss: 0.4096 - val_accuracy: 0.9078\n",
      "Epoch 243/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2800 - accuracy: 0.9439 - val_loss: 0.3938 - val_accuracy: 0.9103\n",
      "Epoch 244/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2843 - accuracy: 0.9417 - val_loss: 0.3978 - val_accuracy: 0.9105\n",
      "Epoch 245/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2776 - accuracy: 0.9446 - val_loss: 0.3925 - val_accuracy: 0.9090\n",
      "Epoch 246/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2770 - accuracy: 0.9455 - val_loss: 0.3989 - val_accuracy: 0.9085\n",
      "Epoch 247/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2823 - accuracy: 0.9424 - val_loss: 0.3984 - val_accuracy: 0.9089\n",
      "Epoch 248/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2777 - accuracy: 0.9447 - val_loss: 0.3976 - val_accuracy: 0.9078\n",
      "Epoch 249/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2803 - accuracy: 0.9416 - val_loss: 0.4018 - val_accuracy: 0.9085\n",
      "Epoch 250/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2757 - accuracy: 0.9430 - val_loss: 0.3976 - val_accuracy: 0.9076\n",
      "Epoch 251/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2815 - accuracy: 0.9418 - val_loss: 0.3837 - val_accuracy: 0.9101\n",
      "Epoch 252/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2770 - accuracy: 0.9441 - val_loss: 0.3996 - val_accuracy: 0.9065\n",
      "Epoch 253/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2789 - accuracy: 0.9441 - val_loss: 0.3937 - val_accuracy: 0.9098\n",
      "Epoch 254/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2786 - accuracy: 0.9429 - val_loss: 0.3993 - val_accuracy: 0.9069\n",
      "Epoch 255/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2756 - accuracy: 0.9442 - val_loss: 0.4006 - val_accuracy: 0.9061\n",
      "Epoch 256/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2732 - accuracy: 0.9450 - val_loss: 0.4016 - val_accuracy: 0.9053\n",
      "Epoch 257/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2741 - accuracy: 0.9438 - val_loss: 0.3956 - val_accuracy: 0.9083\n",
      "Epoch 258/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2810 - accuracy: 0.9417 - val_loss: 0.3907 - val_accuracy: 0.9084\n",
      "Epoch 259/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2782 - accuracy: 0.9414 - val_loss: 0.3939 - val_accuracy: 0.9105\n",
      "Epoch 260/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2747 - accuracy: 0.9446 - val_loss: 0.4035 - val_accuracy: 0.9075\n",
      "Epoch 261/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2746 - accuracy: 0.9458 - val_loss: 0.3942 - val_accuracy: 0.9081\n",
      "Epoch 262/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2691 - accuracy: 0.9462 - val_loss: 0.3921 - val_accuracy: 0.9111\n",
      "Epoch 263/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2753 - accuracy: 0.9434 - val_loss: 0.3908 - val_accuracy: 0.9088\n",
      "Epoch 264/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2726 - accuracy: 0.9451 - val_loss: 0.3971 - val_accuracy: 0.9098\n",
      "Epoch 265/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2655 - accuracy: 0.9462 - val_loss: 0.3904 - val_accuracy: 0.9111\n",
      "Epoch 266/300\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.2716 - accuracy: 0.9438 - val_loss: 0.3934 - val_accuracy: 0.9091\n",
      "Epoch 267/300\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.2680 - accuracy: 0.9456 - val_loss: 0.3898 - val_accuracy: 0.9100\n",
      "Epoch 268/300\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.2745 - accuracy: 0.9443 - val_loss: 0.4064 - val_accuracy: 0.9073\n",
      "Epoch 269/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2670 - accuracy: 0.9468 - val_loss: 0.3972 - val_accuracy: 0.9083\n",
      "Epoch 270/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2694 - accuracy: 0.9444 - val_loss: 0.3916 - val_accuracy: 0.9109\n",
      "Epoch 271/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2711 - accuracy: 0.9448 - val_loss: 0.4128 - val_accuracy: 0.9040\n",
      "Epoch 272/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2665 - accuracy: 0.9468 - val_loss: 0.3909 - val_accuracy: 0.9081\n",
      "Epoch 273/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2598 - accuracy: 0.9472 - val_loss: 0.4069 - val_accuracy: 0.9099\n",
      "Epoch 274/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2613 - accuracy: 0.9482 - val_loss: 0.4013 - val_accuracy: 0.9084\n",
      "Epoch 275/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2672 - accuracy: 0.9457 - val_loss: 0.3969 - val_accuracy: 0.9075\n",
      "Epoch 276/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2613 - accuracy: 0.9482 - val_loss: 0.3990 - val_accuracy: 0.9079\n",
      "Epoch 277/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2675 - accuracy: 0.9451 - val_loss: 0.3966 - val_accuracy: 0.9091\n",
      "Epoch 278/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2635 - accuracy: 0.9472 - val_loss: 0.4005 - val_accuracy: 0.9058\n",
      "Epoch 279/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2647 - accuracy: 0.9467 - val_loss: 0.3911 - val_accuracy: 0.9086\n",
      "Epoch 280/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2546 - accuracy: 0.9492 - val_loss: 0.3872 - val_accuracy: 0.9117\n",
      "Epoch 281/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2603 - accuracy: 0.9455 - val_loss: 0.3874 - val_accuracy: 0.9123\n",
      "Epoch 282/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2638 - accuracy: 0.9457 - val_loss: 0.3887 - val_accuracy: 0.9113\n",
      "Epoch 283/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2613 - accuracy: 0.9468 - val_loss: 0.3938 - val_accuracy: 0.9102\n",
      "Epoch 284/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2639 - accuracy: 0.9458 - val_loss: 0.3980 - val_accuracy: 0.9074\n",
      "Epoch 285/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2612 - accuracy: 0.9463 - val_loss: 0.3930 - val_accuracy: 0.9092\n",
      "Epoch 286/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2557 - accuracy: 0.9496 - val_loss: 0.3933 - val_accuracy: 0.9111\n",
      "Epoch 287/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2567 - accuracy: 0.9478 - val_loss: 0.3972 - val_accuracy: 0.9071\n",
      "Epoch 288/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2610 - accuracy: 0.9486 - val_loss: 0.4046 - val_accuracy: 0.9082\n",
      "Epoch 289/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2590 - accuracy: 0.9484 - val_loss: 0.4050 - val_accuracy: 0.9062\n",
      "Epoch 290/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2584 - accuracy: 0.9473 - val_loss: 0.3976 - val_accuracy: 0.9078\n",
      "Epoch 291/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2513 - accuracy: 0.9498 - val_loss: 0.3947 - val_accuracy: 0.9108\n",
      "Epoch 292/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2518 - accuracy: 0.9518 - val_loss: 0.3922 - val_accuracy: 0.9090\n",
      "Epoch 293/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2547 - accuracy: 0.9485 - val_loss: 0.3982 - val_accuracy: 0.9072\n",
      "Epoch 294/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2528 - accuracy: 0.9502 - val_loss: 0.3960 - val_accuracy: 0.9080\n",
      "Epoch 295/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2514 - accuracy: 0.9498 - val_loss: 0.3924 - val_accuracy: 0.9114\n",
      "Epoch 296/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2576 - accuracy: 0.9482 - val_loss: 0.3891 - val_accuracy: 0.9112\n",
      "Epoch 297/300\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.2548 - accuracy: 0.9488 - val_loss: 0.4000 - val_accuracy: 0.9068\n",
      "Epoch 298/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2510 - accuracy: 0.9500 - val_loss: 0.3932 - val_accuracy: 0.9107\n",
      "Epoch 299/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2540 - accuracy: 0.9496 - val_loss: 0.3887 - val_accuracy: 0.9104\n",
      "Epoch 300/300\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2558 - accuracy: 0.9472 - val_loss: 0.3942 - val_accuracy: 0.9089\n"
     ]
    }
   ],
   "source": [
    "##model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels,batch_size=500,  epochs=300, validation_data=(test_images, test_labels),use_multiprocessing=True,callbacks=[callbacks.LearningRateScheduler(learing_rate_callback, verbose=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 - 4s - loss: 0.3942 - accuracy: 0.9089\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"265.995469pt\" version=\"1.1\" viewBox=\"0 0 385.78125 265.995469\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-05-06T17:32:59.116544</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 265.995469 \r\nL 385.78125 265.995469 \r\nL 385.78125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 378.58125 228.439219 \r\nL 378.58125 10.999219 \r\nL 43.78125 10.999219 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m29d006dd60\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#m29d006dd60\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(55.818182 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"109.896361\" xlink:href=\"#m29d006dd60\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(103.533861 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"160.79329\" xlink:href=\"#m29d006dd60\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(151.24954 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"211.690219\" xlink:href=\"#m29d006dd60\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(202.146469 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"262.587148\" xlink:href=\"#m29d006dd60\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(253.043398 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"313.484078\" xlink:href=\"#m29d006dd60\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 250 -->\r\n      <g transform=\"translate(303.940328 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"364.381007\" xlink:href=\"#m29d006dd60\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 300 -->\r\n      <g transform=\"translate(354.837257 243.037656)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_8\">\r\n     <!-- Epoch -->\r\n     <g transform=\"translate(195.870313 256.715781)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 628 4666 \r\nL 3578 4666 \r\nL 3578 4134 \r\nL 1259 4134 \r\nL 1259 2753 \r\nL 3481 2753 \r\nL 3481 2222 \r\nL 1259 2222 \r\nL 1259 531 \r\nL 3634 531 \r\nL 3634 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1159 525 \r\nL 1159 -1331 \r\nL 581 -1331 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2969 \r\nQ 1341 3281 1617 3432 \r\nQ 1894 3584 2278 3584 \r\nQ 2916 3584 3314 3078 \r\nQ 3713 2572 3713 1747 \r\nQ 3713 922 3314 415 \r\nQ 2916 -91 2278 -91 \r\nQ 1894 -91 1617 61 \r\nQ 1341 213 1159 525 \r\nz\r\nM 3116 1747 \r\nQ 3116 2381 2855 2742 \r\nQ 2594 3103 2138 3103 \r\nQ 1681 3103 1420 2742 \r\nQ 1159 2381 1159 1747 \r\nQ 1159 1113 1420 752 \r\nQ 1681 391 2138 391 \r\nQ 2594 391 2855 752 \r\nQ 3116 1113 3116 1747 \r\nz\r\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 1959 3097 \r\nQ 1497 3097 1228 2736 \r\nQ 959 2375 959 1747 \r\nQ 959 1119 1226 758 \r\nQ 1494 397 1959 397 \r\nQ 2419 397 2687 759 \r\nQ 2956 1122 2956 1747 \r\nQ 2956 2369 2687 2733 \r\nQ 2419 3097 1959 3097 \r\nz\r\nM 1959 3584 \r\nQ 2709 3584 3137 3096 \r\nQ 3566 2609 3566 1747 \r\nQ 3566 888 3137 398 \r\nQ 2709 -91 1959 -91 \r\nQ 1206 -91 779 398 \r\nQ 353 888 353 1747 \r\nQ 353 2609 779 3096 \r\nQ 1206 3584 1959 3584 \r\nz\r\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3122 3366 \r\nL 3122 2828 \r\nQ 2878 2963 2633 3030 \r\nQ 2388 3097 2138 3097 \r\nQ 1578 3097 1268 2742 \r\nQ 959 2388 959 1747 \r\nQ 959 1106 1268 751 \r\nQ 1578 397 2138 397 \r\nQ 2388 397 2633 464 \r\nQ 2878 531 3122 666 \r\nL 3122 134 \r\nQ 2881 22 2623 -34 \r\nQ 2366 -91 2075 -91 \r\nQ 1284 -91 818 406 \r\nQ 353 903 353 1747 \r\nQ 353 2603 823 3093 \r\nQ 1294 3584 2113 3584 \r\nQ 2378 3584 2631 3529 \r\nQ 2884 3475 3122 3366 \r\nz\r\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3513 2113 \r\nL 3513 0 \r\nL 2938 0 \r\nL 2938 2094 \r\nQ 2938 2591 2744 2837 \r\nQ 2550 3084 2163 3084 \r\nQ 1697 3084 1428 2787 \r\nQ 1159 2491 1159 1978 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 4863 \r\nL 1159 4863 \r\nL 1159 2956 \r\nQ 1366 3272 1645 3428 \r\nQ 1925 3584 2291 3584 \r\nQ 2894 3584 3203 3211 \r\nQ 3513 2838 3513 2113 \r\nz\r\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-45\"/>\r\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-70\"/>\r\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-68\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m451b1016d4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m451b1016d4\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(20.878125 232.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m451b1016d4\" y=\"201.259219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.3 -->\r\n      <g transform=\"translate(20.878125 205.058437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-33\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m451b1016d4\" y=\"174.079219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(20.878125 177.878437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m451b1016d4\" y=\"146.899219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.5 -->\r\n      <g transform=\"translate(20.878125 150.698437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m451b1016d4\" y=\"119.719219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(20.878125 123.518437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-36\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m451b1016d4\" y=\"92.539219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.7 -->\r\n      <g transform=\"translate(20.878125 96.338437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 525 4666 \r\nL 3525 4666 \r\nL 3525 4397 \r\nL 1831 0 \r\nL 1172 0 \r\nL 2766 4134 \r\nL 525 4134 \r\nL 525 4666 \r\nz\r\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-37\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m451b1016d4\" y=\"65.359219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(20.878125 69.158437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m451b1016d4\" y=\"38.179219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 0.9 -->\r\n      <g transform=\"translate(20.878125 41.978437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 703 97 \r\nL 703 672 \r\nQ 941 559 1184 500 \r\nQ 1428 441 1663 441 \r\nQ 2288 441 2617 861 \r\nQ 2947 1281 2994 2138 \r\nQ 2813 1869 2534 1725 \r\nQ 2256 1581 1919 1581 \r\nQ 1219 1581 811 2004 \r\nQ 403 2428 403 3163 \r\nQ 403 3881 828 4315 \r\nQ 1253 4750 1959 4750 \r\nQ 2769 4750 3195 4129 \r\nQ 3622 3509 3622 2328 \r\nQ 3622 1225 3098 567 \r\nQ 2575 -91 1691 -91 \r\nQ 1453 -91 1209 -44 \r\nQ 966 3 703 97 \r\nz\r\nM 1959 2075 \r\nQ 2384 2075 2632 2365 \r\nQ 2881 2656 2881 3163 \r\nQ 2881 3666 2632 3958 \r\nQ 2384 4250 1959 4250 \r\nQ 1534 4250 1286 3958 \r\nQ 1038 3666 1038 3163 \r\nQ 1038 2656 1286 2365 \r\nQ 1534 2075 1959 2075 \r\nz\r\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_9\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m451b1016d4\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_17\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(20.878125 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_18\">\r\n     <!-- Accuracy -->\r\n     <g transform=\"translate(14.798438 142.547344)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 2188 4044 \r\nL 1331 1722 \r\nL 3047 1722 \r\nL 2188 4044 \r\nz\r\nM 1831 4666 \r\nL 2547 4666 \r\nL 4325 0 \r\nL 3669 0 \r\nL 3244 1197 \r\nL 1141 1197 \r\nL 716 0 \r\nL 50 0 \r\nL 1831 4666 \r\nz\r\n\" id=\"DejaVuSans-41\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 544 1381 \r\nL 544 3500 \r\nL 1119 3500 \r\nL 1119 1403 \r\nQ 1119 906 1312 657 \r\nQ 1506 409 1894 409 \r\nQ 2359 409 2629 706 \r\nQ 2900 1003 2900 1516 \r\nL 2900 3500 \r\nL 3475 3500 \r\nL 3475 0 \r\nL 2900 0 \r\nL 2900 538 \r\nQ 2691 219 2414 64 \r\nQ 2138 -91 1772 -91 \r\nQ 1169 -91 856 284 \r\nQ 544 659 544 1381 \r\nz\r\nM 1991 3584 \r\nL 1991 3584 \r\nz\r\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2631 2963 \r\nQ 2534 3019 2420 3045 \r\nQ 2306 3072 2169 3072 \r\nQ 1681 3072 1420 2755 \r\nQ 1159 2438 1159 1844 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1341 3275 1631 3429 \r\nQ 1922 3584 2338 3584 \r\nQ 2397 3584 2469 3576 \r\nQ 2541 3569 2628 3553 \r\nL 2631 2963 \r\nz\r\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2194 1759 \r\nQ 1497 1759 1228 1600 \r\nQ 959 1441 959 1056 \r\nQ 959 750 1161 570 \r\nQ 1363 391 1709 391 \r\nQ 2188 391 2477 730 \r\nQ 2766 1069 2766 1631 \r\nL 2766 1759 \r\nL 2194 1759 \r\nz\r\nM 3341 1997 \r\nL 3341 0 \r\nL 2766 0 \r\nL 2766 531 \r\nQ 2569 213 2275 61 \r\nQ 1981 -91 1556 -91 \r\nQ 1019 -91 701 211 \r\nQ 384 513 384 1019 \r\nQ 384 1609 779 1909 \r\nQ 1175 2209 1959 2209 \r\nL 2766 2209 \r\nL 2766 2266 \r\nQ 2766 2663 2505 2880 \r\nQ 2244 3097 1772 3097 \r\nQ 1472 3097 1187 3025 \r\nQ 903 2953 641 2809 \r\nL 641 3341 \r\nQ 956 3463 1253 3523 \r\nQ 1550 3584 1831 3584 \r\nQ 2591 3584 2966 3190 \r\nQ 3341 2797 3341 1997 \r\nz\r\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 2059 -325 \r\nQ 1816 -950 1584 -1140 \r\nQ 1353 -1331 966 -1331 \r\nL 506 -1331 \r\nL 506 -850 \r\nL 844 -850 \r\nQ 1081 -850 1212 -737 \r\nQ 1344 -625 1503 -206 \r\nL 1606 56 \r\nL 191 3500 \r\nL 800 3500 \r\nL 1894 763 \r\nL 2988 3500 \r\nL 3597 3500 \r\nL 2059 -325 \r\nz\r\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-41\"/>\r\n      <use x=\"66.658203\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"121.638672\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"176.619141\" xlink:href=\"#DejaVuSans-75\"/>\r\n      <use x=\"239.998047\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"281.111328\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"342.390625\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"397.371094\" xlink:href=\"#DejaVuSans-79\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#p8b9251051c)\" d=\"M 58.999432 221.160415 \r\nL 60.01737 190.46332 \r\nL 62.053248 155.06409 \r\nL 63.071186 142.468873 \r\nL 65.107063 122.785116 \r\nL 67.14294 111.15209 \r\nL 68.160879 105.830244 \r\nL 69.178818 101.144399 \r\nL 70.196756 98.964568 \r\nL 71.214695 100.019158 \r\nL 73.250572 90.364818 \r\nL 74.268511 88.483967 \r\nL 75.286449 87.048858 \r\nL 76.304388 83.575251 \r\nL 77.322326 83.537196 \r\nL 79.358203 80.3517 \r\nL 80.376142 79.405847 \r\nL 81.394081 78.199053 \r\nL 82.412019 78.160998 \r\nL 83.429958 76.551943 \r\nL 84.447896 77.095536 \r\nL 85.465835 76.388869 \r\nL 86.483774 74.165539 \r\nL 87.501712 74.301445 \r\nL 89.537589 73.105521 \r\nL 90.555528 71.893283 \r\nL 91.573466 72.697818 \r\nL 92.591405 71.920468 \r\nL 93.609344 72.431449 \r\nL 94.627282 71.534522 \r\nL 96.663159 70.876764 \r\nL 98.699037 69.207914 \r\nL 99.716975 69.762377 \r\nL 100.734914 68.860007 \r\nL 101.752852 70.017876 \r\nL 102.770791 69.218784 \r\nL 103.788729 69.251396 \r\nL 104.806668 68.370767 \r\nL 106.842545 68.354453 \r\nL 107.860484 67.799989 \r\nL 108.878422 67.838028 \r\nL 109.896361 67.495565 \r\nL 110.9143 58.493555 \r\nL 111.932238 53.927303 \r\nL 112.950177 51.3126 \r\nL 113.968115 51.040787 \r\nL 114.986054 50.502638 \r\nL 116.003992 49.007733 \r\nL 117.021931 48.442382 \r\nL 118.03987 47.387808 \r\nL 119.057808 47.045328 \r\nL 120.075747 46.045107 \r\nL 122.111624 46.25168 \r\nL 123.129563 45.343867 \r\nL 124.147501 45.050329 \r\nL 125.16544 45.338423 \r\nL 126.183378 44.251237 \r\nL 127.201317 43.805479 \r\nL 128.219255 43.761997 \r\nL 131.273071 42.680222 \r\nL 132.29101 42.71285 \r\nL 134.326887 42.093147 \r\nL 135.344826 42.299704 \r\nL 137.380703 40.723261 \r\nL 138.398641 41.549537 \r\nL 140.434518 40.772187 \r\nL 141.452457 40.717834 \r\nL 142.470396 40.217723 \r\nL 143.488334 40.467779 \r\nL 144.506273 39.826334 \r\nL 145.524211 40.005707 \r\nL 147.560089 39.103337 \r\nL 148.578027 39.663244 \r\nL 149.595966 38.967446 \r\nL 150.613904 38.815226 \r\nL 151.631843 38.853281 \r\nL 152.649781 38.445578 \r\nL 153.66772 38.364041 \r\nL 154.685659 38.135727 \r\nL 155.703597 38.26619 \r\nL 156.721536 37.554079 \r\nL 159.775352 38.097672 \r\nL 160.79329 37.189874 \r\nL 161.811229 37.738911 \r\nL 162.829167 37.059395 \r\nL 163.847106 37.249671 \r\nL 164.865044 37.168117 \r\nL 165.882983 36.559284 \r\nL 167.91886 36.994172 \r\nL 168.936799 36.39621 \r\nL 169.954737 35.98308 \r\nL 170.972676 36.004821 \r\nL 171.990615 36.559284 \r\nL 173.008553 35.803691 \r\nL 175.04443 35.249211 \r\nL 176.062369 35.200286 \r\nL 177.080307 35.78195 \r\nL 178.098246 34.988285 \r\nL 179.116185 35.075266 \r\nL 180.134123 34.640379 \r\nL 181.152062 34.754544 \r\nL 182.17 34.580583 \r\nL 183.187939 34.966544 \r\nL 184.205878 34.787156 \r\nL 185.223816 34.12397 \r\nL 187.259693 34.216378 \r\nL 188.277632 34.629508 \r\nL 189.29557 34.836081 \r\nL 190.313509 33.933695 \r\nL 191.331448 34.03699 \r\nL 192.349386 33.792361 \r\nL 193.367325 33.950009 \r\nL 194.385263 33.335749 \r\nL 195.403202 33.172658 \r\nL 196.421141 33.390102 \r\nL 197.439079 33.297694 \r\nL 198.457018 33.77062 \r\nL 199.474956 33.178102 \r\nL 200.492895 33.091121 \r\nL 201.510833 33.319435 \r\nL 202.528772 32.585567 \r\nL 203.546711 32.009362 \r\nL 204.564649 33.243325 \r\nL 205.582588 32.455104 \r\nL 206.600526 33.101991 \r\nL 207.618465 32.525787 \r\nL 208.636404 32.3029 \r\nL 210.672281 32.17788 \r\nL 212.708158 32.438806 \r\nL 213.726096 31.938695 \r\nL 214.744035 32.259417 \r\nL 215.761974 31.748436 \r\nL 216.779912 32.128954 \r\nL 217.797851 31.558177 \r\nL 218.815789 31.846271 \r\nL 219.833728 31.498381 \r\nL 220.851667 31.704938 \r\nL 222.887544 31.052623 \r\nL 223.905482 30.835179 \r\nL 224.923421 30.878678 \r\nL 225.941359 31.1994 \r\nL 226.959298 30.987383 \r\nL 227.977237 31.025438 \r\nL 228.995175 31.389659 \r\nL 230.013114 30.64492 \r\nL 231.031052 31.030882 \r\nL 232.048991 30.802567 \r\nL 233.06693 30.824309 \r\nL 234.084868 29.927366 \r\nL 235.102807 30.786253 \r\nL 236.120745 30.394865 \r\nL 237.138684 30.177421 \r\nL 238.156622 30.981956 \r\nL 240.1925 29.927366 \r\nL 241.210438 30.655791 \r\nL 242.228377 29.851272 \r\nL 243.246315 30.253531 \r\nL 244.264254 30.237217 \r\nL 246.300131 29.873013 \r\nL 247.31807 29.595773 \r\nL 249.353947 29.661012 \r\nL 250.371885 29.780605 \r\nL 251.389824 29.242439 \r\nL 252.407763 29.487051 \r\nL 253.425701 29.606643 \r\nL 254.44364 29.438125 \r\nL 255.461578 28.889089 \r\nL 257.497456 29.041309 \r\nL 258.515394 29.47618 \r\nL 259.533333 29.057607 \r\nL 260.551271 29.209811 \r\nL 261.56921 28.628163 \r\nL 263.605087 28.916273 \r\nL 264.623026 28.829293 \r\nL 265.640964 28.176978 \r\nL 266.658903 29.111976 \r\nL 267.676841 29.05218 \r\nL 268.69478 28.29657 \r\nL 269.712719 28.840163 \r\nL 270.730657 28.644477 \r\nL 271.748596 29.014125 \r\nL 272.766534 28.405292 \r\nL 275.82035 27.769275 \r\nL 277.856227 28.7097 \r\nL 278.874166 28.182421 \r\nL 279.892104 28.57381 \r\nL 280.910043 27.655126 \r\nL 281.927982 28.280256 \r\nL 282.94592 27.769275 \r\nL 283.963859 27.992162 \r\nL 284.981797 27.861683 \r\nL 285.999736 27.443126 \r\nL 287.017674 27.812774 \r\nL 288.035613 27.426812 \r\nL 289.053552 28.117182 \r\nL 290.07149 27.24198 \r\nL 291.089429 26.866905 \r\nL 292.107367 27.584459 \r\nL 293.125306 27.182184 \r\nL 294.143245 27.372443 \r\nL 295.161183 27.155015 \r\nL 296.179122 27.600757 \r\nL 298.214999 27.345274 \r\nL 300.250876 26.866905 \r\nL 301.268815 27.078905 \r\nL 302.286753 26.845164 \r\nL 303.304692 27.11696 \r\nL 304.32263 27.013682 \r\nL 305.340569 26.605979 \r\nL 306.358508 26.877775 \r\nL 307.376446 26.470073 \r\nL 308.394385 26.268943 \r\nL 309.412323 26.682073 \r\nL 310.430262 26.524442 \r\nL 311.4482 26.812536 \r\nL 313.484078 26.769054 \r\nL 314.502016 26.546183 \r\nL 315.519955 26.497257 \r\nL 316.537893 26.948442 \r\nL 318.573771 26.393979 \r\nL 319.591709 26.399406 \r\nL 320.609648 26.774497 \r\nL 321.627586 26.763626 \r\nL 322.645525 26.258072 \r\nL 323.663463 26.274386 \r\nL 324.681402 25.806887 \r\nL 325.699341 26.13848 \r\nL 326.717279 25.709036 \r\nL 327.735218 26.089554 \r\nL 329.771095 25.964535 \r\nL 330.789034 26.14935 \r\nL 331.806972 25.475294 \r\nL 332.824911 26.35048 \r\nL 333.842849 25.72535 \r\nL 334.860788 25.942777 \r\nL 335.878726 25.44811 \r\nL 336.896665 25.420925 \r\nL 337.914604 25.676424 \r\nL 338.932542 25.230666 \r\nL 339.950481 25.730777 \r\nL 341.986358 25.774259 \r\nL 343.004297 25.290462 \r\nL 344.022235 25.812314 \r\nL 345.040174 25.415498 \r\nL 346.058112 25.839499 \r\nL 347.076051 25.834055 \r\nL 348.093989 25.507906 \r\nL 349.111928 24.643575 \r\nL 350.129867 25.676424 \r\nL 351.147805 25.024093 \r\nL 352.165744 25.002352 \r\nL 353.183682 25.241537 \r\nL 354.201621 25.111074 \r\nL 355.21956 24.589222 \r\nL 356.237498 25.366573 \r\nL 357.255437 24.871889 \r\nL 358.273375 24.638148 \r\nL 359.291314 25.138258 \r\nL 361.327191 24.610963 \r\nL 362.34513 24.822963 \r\nL 363.363068 24.68163 \r\nL 363.363068 24.68163 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#p8b9251051c)\" d=\"M 58.999432 249.177559 \r\nL 60.01737 238.414281 \r\nL 61.035309 213.191239 \r\nL 62.053248 202.917195 \r\nL 63.071186 154.264998 \r\nL 64.089125 167.202678 \r\nL 65.107063 119.202804 \r\nL 67.14294 112.135998 \r\nL 68.160879 133.852825 \r\nL 69.178818 96.181331 \r\nL 70.196756 88.706838 \r\nL 71.214695 108.847223 \r\nL 72.232633 102.4871 \r\nL 73.250572 88.462226 \r\nL 74.268511 79.411274 \r\nL 75.286449 97.078274 \r\nL 76.304388 89.848393 \r\nL 77.322326 96.398774 \r\nL 78.340265 95.773644 \r\nL 79.358203 82.237994 \r\nL 80.376142 78.976403 \r\nL 81.394081 90.881242 \r\nL 82.412019 79.302553 \r\nL 83.429958 82.890325 \r\nL 84.447896 84.439583 \r\nL 85.465835 74.709132 \r\nL 86.483774 81.313882 \r\nL 87.501712 75.225556 \r\nL 88.519651 74.790685 \r\nL 89.537589 78.07946 \r\nL 90.555528 72.398837 \r\nL 91.573466 81.069254 \r\nL 92.591405 68.02286 \r\nL 93.609344 93.59924 \r\nL 94.627282 84.439583 \r\nL 95.645221 81.748753 \r\nL 96.663159 66.120252 \r\nL 97.681098 96.398774 \r\nL 98.699037 90.392002 \r\nL 99.716975 77.916386 \r\nL 100.734914 68.050045 \r\nL 101.752852 75.334278 \r\nL 102.770791 74.980944 \r\nL 103.788729 74.138354 \r\nL 104.806668 100.502955 \r\nL 105.824607 75.877871 \r\nL 106.842545 60.575536 \r\nL 107.860484 67.560804 \r\nL 108.878422 70.061358 \r\nL 109.896361 73.159874 \r\nL 110.9143 52.231284 \r\nL 111.932238 49.268659 \r\nL 112.950177 50.111233 \r\nL 113.968115 48.290178 \r\nL 114.986054 49.295843 \r\nL 116.003992 45.599365 \r\nL 117.021931 46.278865 \r\nL 118.03987 46.224496 \r\nL 119.057808 47.094254 \r\nL 120.075747 44.457794 \r\nL 121.093685 44.919866 \r\nL 122.111624 46.632198 \r\nL 123.129563 44.484979 \r\nL 124.147501 44.185998 \r\nL 125.16544 44.947034 \r\nL 126.183378 42.310574 \r\nL 127.201317 44.811144 \r\nL 128.219255 41.875703 \r\nL 129.237194 42.310574 \r\nL 130.255133 44.131645 \r\nL 131.273071 43.12598 \r\nL 132.29101 42.63674 \r\nL 134.326887 41.902871 \r\nL 135.344826 40.435151 \r\nL 136.362764 42.120315 \r\nL 137.380703 42.473665 \r\nL 138.398641 41.223372 \r\nL 139.41658 42.419296 \r\nL 140.434518 42.364943 \r\nL 141.452457 41.440816 \r\nL 142.470396 41.821334 \r\nL 144.506273 40.625426 \r\nL 146.54215 39.864373 \r\nL 147.560089 41.549537 \r\nL 148.578027 40.706963 \r\nL 149.595966 40.625426 \r\nL 150.613904 42.065962 \r\nL 151.631843 40.163354 \r\nL 153.66772 40.163354 \r\nL 154.685659 42.799814 \r\nL 155.703597 40.625426 \r\nL 156.721536 41.95724 \r\nL 157.739474 39.402318 \r\nL 158.757413 42.364943 \r\nL 159.775352 41.332094 \r\nL 160.79329 41.087482 \r\nL 161.811229 39.483855 \r\nL 162.829167 38.967446 \r\nL 163.847106 40.788501 \r\nL 164.865044 39.239243 \r\nL 165.882983 39.048984 \r\nL 166.900922 40.598242 \r\nL 167.91886 41.141835 \r\nL 168.936799 39.429502 \r\nL 169.954737 39.266411 \r\nL 170.972676 41.169019 \r\nL 171.990615 39.429502 \r\nL 173.008553 39.565392 \r\nL 174.026492 39.483855 \r\nL 175.04443 39.103337 \r\nL 176.062369 38.586912 \r\nL 177.080307 39.646946 \r\nL 178.098246 39.456687 \r\nL 179.116185 39.021799 \r\nL 180.134123 39.945927 \r\nL 181.152062 39.755651 \r\nL 182.17 38.804356 \r\nL 183.187939 38.505375 \r\nL 184.205878 38.750003 \r\nL 185.223816 38.505375 \r\nL 187.259693 39.347965 \r\nL 188.277632 40.299261 \r\nL 189.29557 39.592577 \r\nL 190.313509 38.451022 \r\nL 191.331448 39.646946 \r\nL 192.349386 37.255098 \r\nL 193.367325 39.266411 \r\nL 194.385263 39.864373 \r\nL 195.403202 38.668465 \r\nL 197.439079 37.961782 \r\nL 198.457018 38.695634 \r\nL 199.474956 37.744338 \r\nL 200.492895 39.184874 \r\nL 201.510833 37.689985 \r\nL 202.528772 38.695634 \r\nL 203.546711 37.36382 \r\nL 204.564649 37.255098 \r\nL 205.582588 38.858725 \r\nL 206.600526 38.586912 \r\nL 207.618465 39.266411 \r\nL 208.636404 37.581263 \r\nL 209.654342 38.423837 \r\nL 210.672281 37.771522 \r\nL 211.690219 37.391004 \r\nL 212.708158 38.668465 \r\nL 213.726096 37.717153 \r\nL 214.744035 37.635616 \r\nL 216.779912 38.233578 \r\nL 217.797851 38.206394 \r\nL 219.833728 38.885893 \r\nL 220.851667 38.641281 \r\nL 221.869605 40.00028 \r\nL 222.887544 38.994615 \r\nL 223.905482 37.526894 \r\nL 224.923421 37.17356 \r\nL 225.941359 38.505375 \r\nL 226.959298 38.559744 \r\nL 227.977237 36.657136 \r\nL 228.995175 36.439692 \r\nL 230.013114 36.466876 \r\nL 231.031052 37.771522 \r\nL 232.048991 37.227913 \r\nL 233.06693 36.548414 \r\nL 234.084868 38.967446 \r\nL 235.102807 38.451022 \r\nL 236.120745 37.499726 \r\nL 237.138684 36.358155 \r\nL 238.156622 37.418172 \r\nL 239.174561 38.695634 \r\nL 240.1925 38.233578 \r\nL 241.210438 36.68432 \r\nL 242.228377 36.276617 \r\nL 244.264254 39.347965 \r\nL 245.282193 37.227913 \r\nL 246.300131 35.515581 \r\nL 247.31807 36.711505 \r\nL 248.336008 36.113543 \r\nL 249.353947 37.119191 \r\nL 250.371885 37.771522 \r\nL 251.389824 37.934597 \r\nL 252.407763 36.983301 \r\nL 253.425701 37.010486 \r\nL 254.44364 37.825875 \r\nL 255.461578 38.369484 \r\nL 256.479517 37.418172 \r\nL 257.497456 37.499726 \r\nL 258.515394 36.629951 \r\nL 259.533333 36.140711 \r\nL 260.551271 39.076152 \r\nL 261.56921 36.358155 \r\nL 263.605087 36.494061 \r\nL 264.623026 37.608432 \r\nL 265.640964 38.070503 \r\nL 266.658903 35.70584 \r\nL 267.676841 35.923283 \r\nL 269.712719 37.146376 \r\nL 270.730657 37.771522 \r\nL 271.748596 36.928932 \r\nL 272.766534 37.064839 \r\nL 273.784473 36.249433 \r\nL 274.802411 38.614096 \r\nL 275.82035 36.303802 \r\nL 276.838289 36.466876 \r\nL 277.856227 35.597118 \r\nL 279.892104 37.064839 \r\nL 280.910043 36.820227 \r\nL 281.927982 36.358155 \r\nL 282.94592 35.733024 \r\nL 283.963859 36.575598 \r\nL 284.981797 36.385339 \r\nL 285.999736 36.330986 \r\nL 287.017674 37.391004 \r\nL 288.035613 35.542765 \r\nL 289.053552 36.113543 \r\nL 290.07149 35.977636 \r\nL 291.089429 37.17356 \r\nL 292.107367 36.956117 \r\nL 293.125306 36.222265 \r\nL 294.143245 36.629951 \r\nL 295.161183 36.439692 \r\nL 296.179122 37.064839 \r\nL 297.19706 36.032005 \r\nL 298.214999 36.358155 \r\nL 299.232937 36.19508 \r\nL 300.250876 36.358155 \r\nL 301.268815 36.032005 \r\nL 302.286753 36.602783 \r\nL 303.304692 37.554079 \r\nL 304.32263 36.059174 \r\nL 305.340569 35.379674 \r\nL 306.358508 35.325321 \r\nL 307.376446 35.733024 \r\nL 308.394385 35.868914 \r\nL 309.412323 35.760193 \r\nL 310.430262 36.059174 \r\nL 311.4482 35.868914 \r\nL 312.466139 36.113543 \r\nL 313.484078 35.434043 \r\nL 314.502016 36.412524 \r\nL 315.519955 35.515581 \r\nL 316.537893 36.303802 \r\nL 318.573771 36.738673 \r\nL 319.591709 35.923283 \r\nL 320.609648 35.896099 \r\nL 321.627586 35.325321 \r\nL 322.645525 36.140711 \r\nL 323.663463 35.977636 \r\nL 324.681402 35.162247 \r\nL 325.699341 35.787377 \r\nL 327.735218 35.162247 \r\nL 328.753156 35.70584 \r\nL 329.771095 35.461212 \r\nL 330.789034 36.19508 \r\nL 331.806972 35.923283 \r\nL 332.824911 35.2166 \r\nL 333.842849 37.092023 \r\nL 334.860788 35.977636 \r\nL 335.878726 35.488396 \r\nL 336.896665 35.896099 \r\nL 337.914604 36.140711 \r\nL 338.932542 36.032005 \r\nL 339.950481 35.70584 \r\nL 340.968419 36.602783 \r\nL 343.004297 34.999156 \r\nL 344.022235 34.836081 \r\nL 346.058112 35.406859 \r\nL 347.076051 36.167895 \r\nL 349.111928 35.162247 \r\nL 350.129867 36.249433 \r\nL 351.147805 35.950452 \r\nL 352.165744 36.494061 \r\nL 353.183682 36.059174 \r\nL 354.201621 35.243784 \r\nL 356.237498 36.222265 \r\nL 357.255437 36.004821 \r\nL 358.273375 35.080693 \r\nL 359.291314 35.135062 \r\nL 360.309252 36.330986 \r\nL 361.327191 35.270952 \r\nL 362.34513 35.352506 \r\nL 363.363068 35.760193 \r\nL 363.363068 35.760193 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 43.78125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 378.58125 228.439219 \r\nL 378.58125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 378.58125 228.439219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 10.999219 \r\nL 378.58125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 274.6375 223.439219 \r\nL 371.58125 223.439219 \r\nQ 373.58125 223.439219 373.58125 221.439219 \r\nL 373.58125 192.804844 \r\nQ 373.58125 190.804844 371.58125 190.804844 \r\nL 274.6375 190.804844 \r\nQ 272.6375 190.804844 272.6375 192.804844 \r\nL 272.6375 221.439219 \r\nQ 272.6375 223.439219 274.6375 223.439219 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 276.6375 198.903281 \r\nL 296.6375 198.903281 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_19\">\r\n     <!-- accuracy -->\r\n     <g transform=\"translate(304.6375 202.403281)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-75\"/>\r\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-79\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_21\">\r\n     <path d=\"M 276.6375 213.581406 \r\nL 296.6375 213.581406 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_22\"/>\r\n    <g id=\"text_20\">\r\n     <!-- val_accuracy -->\r\n     <g transform=\"translate(304.6375 217.081406)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 191 3500 \r\nL 800 3500 \r\nL 1894 563 \r\nL 2988 3500 \r\nL 3597 3500 \r\nL 2284 0 \r\nL 1503 0 \r\nL 191 3500 \r\nz\r\n\" id=\"DejaVuSans-76\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 603 4863 \r\nL 1178 4863 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\r\n       <path d=\"M 3263 -1063 \r\nL 3263 -1509 \r\nL -63 -1509 \r\nL -63 -1063 \r\nL 3263 -1063 \r\nz\r\n\" id=\"DejaVuSans-5f\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-76\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-5f\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"259.521484\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"314.501953\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"369.482422\" xlink:href=\"#DejaVuSans-75\"/>\r\n      <use x=\"432.861328\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"473.974609\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"535.253906\" xlink:href=\"#DejaVuSans-63\"/>\r\n      <use x=\"590.234375\" xlink:href=\"#DejaVuSans-79\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p8b9251051c\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"10.999219\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAA9rklEQVR4nO3dd5hdVbn48e97yvSemfQKpBNCCl0hEKrSFCOgICCCDaXcq0S8IiKWq/5QuBdL8NJUjBSBiBQpQXpJIKQ3Uidlep85c9r7+2PtmTmZzCSTMCeTyXk/zzPPnL3OLmufM7Pevdbaey1RVYwxxqQuX19nwBhjTN+yQGCMMSnOAoExxqQ4CwTGGJPiLBAYY0yKs0BgjDEpLmmBQETuE5FyEVnezfsiIneLyHoRWSoi05OVF2OMMd1LZo3gAeDsPbx/DjDW+7kW+F0S82KMMaYbSQsEqvoqUL2HVS4AHlLnbaBARIYkKz/GGGO6FujDYw8DtiYsl3ppOzqvKCLX4moNZGdnz5gwYcIByaAxxhwqFi9eXKmqJV2915eBoMdUdR4wD2DmzJm6aNGiPs6RMcb0LyKyubv3+vKuoW3AiITl4V6aMcaYA6gvA8EC4Eve3UPHA3WquluzkDHGmORKWtOQiPwVmAUUi0gp8EMgCKCqvweeAT4FrAeagauSlRdjjDHdS1ogUNVL9/K+At9M1vGNMcb0jD1ZbIwxKc4CgTHGpDgLBMYYk+IsEBhjTIqzQGCMMSnOAoExxqQ4CwTGGJPi+sVYQ8YYczCLxOK0RGLkZQQBUFVicSXg3/Va+92N1fzlnc1cP3ssy7fXE43FObwkh1U76vnnsh1ccsxIVu2oZ/XOBj5xxAAqG8Ms2VrLwNx06kMRLj9hNKeM63LcuI/FAoEx5pBR2xwmPzOIiOzTds3hKJUNYV5ZW044Gqe2OUIkHmfKsHwaQlEygj4Wrq6gpjnM6RMHsbasgZ11Id7eUEVWegABqprCzBpXQn5mkDVlDawra2TqiHy2Vrcwa3wJzeEYzy7fQSSmPLVk+255yAz6eW1dJX6fMCg3nRdXleH3CeMH5bKuvIHCrDSaWqO99EntygKBMaZPqSpvb6hmXXkDFxw9jIqGEAVZaVQ2tjJuYC4+n1DbHGbRphpW76ynJDedHXUhxhRnU9cS4aVV5QzITuPtDVVsrwsxakAWGQE/o4uzGJCTTmskzvryBiobwxw9ogCfT3hjfSU1zWHGD8plwuBcXlpVTkNCIev3CT6BSEzb09L8PkTgtXWVZAb9ZAR9fHb6cBpbozS1RhmUl8GbH1USjsXJTQ/yqSmDWbG9nklD8/jnsh1kBP18bsZwzpw0mD+/vZkrThzN0IIM1pc3Mjg/kyMG5vDUkm3MGj+QofkZrClrYFhBJrleLSOZxI300H/YMNTGHFhVja2kB/3kpAeIxOII7NLkUV4f4sPSOpaW1hL0+7j02JHkpAdoCEV4e2M1r6+r4JjRRaze2cDgvAze21TN6p0NDC/MZGd9iFFFWSxcUwG4AjgW7yiTBudlEIrGqG2OdJu/kUVZ1IciTBtRwIxRhby7qYaAT1hf3khzOEaaXxg1IJvi3HReWVNORtDPJ48oZkhBBm99VEVZfStTR+Qza9xAJgzJpSg7jaDfR5rfR3lDK3mZARpCUXIzAtQ2R9ha3czpEwcB4PPtW82jL4nIYlWd2eV7FgiMSQ21zWGy0wME/T5awjH+/kEpza0xCrKCHH/YAMrqQzSFY/zjw+00hCKMHpBNOBZn/rtbKc5N44bZ47j75XW0RuLMGFVIdrqfDRVNLNpcA7hCPK5K0O8jO81PTafCO+ATonGlOCeNyUPz2VrTTNDnY01ZAzedMY5pIwt4ZtlOjhqeT1NrlMw0P2+sr6QoO40RhVkcPaKASUPzqGhoZVBeBpuqmshNDzKiKLPHTUHxuCLCPjcdHQosEBjTj1U3hXllTTnnTR1KOBqnuinMiKIsyhtCvLyqnE+MLeaFlWVMGZZPTkaA9eWNLN9Wz/uba4ipkhn0E/ALr66tID3gZ0RRJjvqQjSEum5vzgz6GVaYyZaqZhTlE0cUs2RrLTXNEYbmZzDMu5KvbYqQFvBxzcmHMXNUIZOH5rOzPsQfX9tAVWOYCUNyaYnEuPSYkWyvbeGYMUU0t8bIywy0F8TRWJzttSFGDsg6kB9pSrJAYMxBZn15I6+vq2B7XYiBuekcPjCHsroQxTnpNEdivLyqjPGD89hR18K/11awuaqZUQOyKK1pIRZXTh5XwrsbqwhF4ohA53/jgE84clg+2el+WsIxapojnDKuBBEorWmhOCedC44eyqSheZTXh3h+RRmD8zIYlJfBuEE5DMzL2OXqubY5TGlNC4eX5JCZ5gcgHHXHDvrtLvT+YE+BwDqLjekBVeXdjdWMLs5mUF5Ge3ooEqOioZX6UISMoJ9hBZks2lRDQyjC9roQdc1hKpvCbKlqRgTK61uJxuN8VNEEQFrARzga3+14mUE/Ty7ZTm5GgOKcdK6fPZZX1pRz9uTBtERi/PntzZw/dShnTh7Mkx9s48oTR1PbEiEWV8YUZ3PEwBwygv4enVteRpAjBubulp7Y/l2QlUZBVtou76cFLAAcKqxGYAzQEo6xqaqJwXkZ+HxCXkaA0poWNlY2kZ0e4DcvruW1dZWMHpBFcU46A/PSaWyN8eb6SqLx7v+HfOLazscUZxONKSMHZBGLK2dMGsRpEwYyrCCTbbUtbKluZmRRFhsrmyirb+W8qUNoCEUZkJ3WZXt2KBLrcUFvDFiNwKSweFxZUlqLKuRnBji8JIdo3N3H3RqNsbGiiWhc+dt7W2mJxNq3Gz0gi83Vze1NLllpfr52yuH83+sbqG4Ks2xbHVlpfr7yycM4rDibvMwAzeEYW6tbGFqQwejibDICfoYUZKAKJbnp3eZxeGEWwwuz2l+3Sc/pvqC3IGB6kwUC0y/F4sqW6mYCPiE/K8hLq8qoagzz+vpK0vw+apsjzBxdyKJNNby7qbp9uzHF2eRlBPiwtA7oaJo5c9Igzp06lJ11LURiyitryjl1wkDOnjyYnfUhjh5RwKgB2Zw/dSgDctLICPoJ+oWsNPsX6nPxGERDkJa9/9sj4PNBUyWID7KKoH4H+NMge0DHutGwO1ZG3p732VQJyx6DUC2MOglGHAeBNIhFoW4LFB226/rN1e6Y4Dp8trzl8jH8WAg3ggik795811uS2jQkImcDdwF+4I+q+vNO748C7gNKgGrgMlUt3dM+rWko9awta2Dx5hoG5qZTnJPOH179iDfWV1HX4m5PLMpOo7opDMBhJdn4RMgI+li+rZ5Beel8Y9YRjBqQxY66EP9cuoONlU1849TDOWVcCYO99v7OQwEc1FRdwZAo3NRRENZvh79fC5/8D8gaAAMngr8HDyXVbIbsYghkumVfwmey6D7YuRwmnQ+Hzdp1u3gMYhFXaMZa4b0/wo6lcNZPIGegy+/KJ2H7Ejjl5o68bHkL/v0LmDIHcgbBuLNceqgWMgvd61gUPnoZBk6AcDOk50D5ahh5vDvfxQ/AC7dCaz0MnQ4jjoVICxSPc3kJZsPi++GwU6FyDQw5Gk69xeVh7fNQugjefwjyh8O0L8KLPwKNQ9EY2PEh+NNhwBGQO8jl89//DfE4fGE+vPpL95lNvhCKx8POZRBpgsNnwxNfhfptHZ9RZhEcey0sewSqN8Cp34eCkTDmFHjjLnjndzD6k3Dc12DNs7Dkz267w0+DTa9DLOwCyqy5MObkvX+XXeiTu4ZExA+sBc4ASoH3gEtVdWXCOo8CT6vqgyJyGnCVql6+p/1aIDi0NIejtIRjROPK+vJGBuSk8cT72/hgSy2ThuaxrbaFF1aW7bLNgOw0Zk8cyMzRRWypambhmnK+c9Z4DivO2eU2xB61o4fq93511yYec1d6WUU9K1h7orXB/eQNha3vwjPfcQVF4Sj4wiOw+p+w7l9w0f+5fH7wZ/jXD+Azf4BxZ0LDTnjsy7D5Tfj8gzDpAre8/HF3RalxGHc2zHkQAumw9jloroJx53Rc6YbqYMG3XWE9/UuuANy5DDLyYdrlcOK34deTIB51+5t4Ppz1U2gqh23vw7v3uqvdodNcoQqAQHYJHHctrHkOtnn/s2m5EG5wBWNajrs6bnPc16FuK6x+GgpHuyBQMMIFjM5yBsNRc+DN/3GF6agTXQFatd59Ny01HetmFrplf5orUNNyYMDh7jzBBZDKte7Ke9CRroBurnafb80mqN0K2xa7YJM7FBq2gy/gAmbRaPdZtRNA3T7mPOAC0sZXXdDY/gEMPgoyC1xaokkXuO+/YYdbPukGiDTDu/Ng2Ew44nRYOh/O/AlMPHdf/8pczvooEJwA3KaqZ3nL3wNQ1Z8lrLMCOFtVt4rrEatT1T3+V1og6H9icWVDhXvKc/57W1hf3sj4wbmMHZjL/W9sZHtdiKBPaAq7NnoROHJoPuvLGxGBr59yOBccPYwPttawZGst188eu9sdLPuleiP89ng4+2cw88surbna/ZMOmQpL/wZHnAFrn4UBY+H1O6FiNUy60BW64K5GfUEXHN77o/vHH3uGK5QyC932g490hWLOwF2Pv/gBePZmQODqf8Hfr3GF8sTz4IO/uEI32uLWPeeXbvvHvgw+v9vmqmfg1V/Bhlcgfxi01MKFv4WHPw8zroLazZA3DD74k8vXwImw8im3v7zhMPtWeP3XrrCLR9yVb+U60BhM/qwrNFc/7a52K9fA1153V9Gv/sp9SdFWt25bAQvuqnbKHBg+Ex67GipWuf2e8E2Xl5VPuUJyycMuf5//k2smefN/XEHnT4PpV7ir6dYG2PQanPwd9/llFLgCPW8ovHQ7VK2D4cfAl5/3PpME9dtdgd9Y7q72VzwBg6e4gnbdv1wQGHWSq6EEM9x6TZXuc/J30dzXWA51pa4gf/QKl6+rnoVBk92x6ra54LX9fffdz761o6kHXJNS+QpXI4mF4aOFrnbz0UJXExpxrPs8N78BhWNcjSQeg1X/gMNPdUE5Hgd093Ptob4KBJ/DFfJf8ZYvB45T1esS1nkYeEdV7xKRzwKPA8WqWtXdfi0QHLw+qmikMCuNFdvreOujKprDMd7eUMXasgbabqzx+4SjhrtCviEUpSAryCnjShjQsJpPTyxiZ+5kprKW4SVFRAcdRTSuZEjU/fMktpG21LqCdPObcN5vXAGy/QNXLW8sg5d/7Kr9+cNcW+/Y011hX3SYu4L7+1fdP+LWd9zV5Tfegn/eBKuedoVi25VdIMO1CYMr8Cee6wqVUZ9wzQXLH+/I0+CjYOdS97p4vCu0msq9N8VdtRaOdv/UR38R7j3VtR1XrnU1k2iLu3KffCGsXOAK6eO/4ZoNqje4IDH8GPjsvfDgea4AikfcVeLhp8EfTnZ5Ts+FG5a78wNY/6KrRVSug+O+ChPOdU0XtZshqxgmf8b9RFvgzxe55pT/XOuaXl68Dd7+nWsO+uIjbn+1W9z+ROC0H7iAd89x0FQB336/o/07FnGBNXfQ7n8sddvclf6RF7n9xGPusysc3dEspOoK4K62b6pygfnYa9w2B1I07K7WMwsO7HE/poM5EAwF/hcYA7wKXAQcqaq1nfZ1LXAtwMiRI2ds3rw5KXk2PVfV2IrfJ/z9/W1IUxnx5U/xSOVINvpHE47GCfiE4kALpxRWMXVEIdnDJpGeW8RhJTmMe++HaLiB7afdTW5GgLw3/9tVnQMZrrD86GVXsH/1Ndj8OjzzXVe4feJGePNuV5DWbXNtwL6gK/DCTS5YjD3LXbU3VcDoT7jCMqsINr3hrrBP/BYsfQQad7oTKZnorlozi1zb9HFfd4Xq4vth4CRXGI8+yQWLw091BddDF7or1pqNrqYwa64rNMef484jEnJtwAA7PnDtyJXrXACp3w6tdRDMcles33jL7eudP7j2/DNu3739f+UCVxOYdplrd0/LhrIV8Pz3Xe1hxlWuPf/fv4SFd8CsW2DWzbt/afF4R7t/JOSC2KgT3dUnuKaYu45yNZrz7urYLlTnvo9gZvd/EMv/7gLEJ27Yj78mcyActE1DndbPAVar6vA97ddqBAdIzSaYfxkNJ83lzs1jOG98Nv6MfBYs3cGGikb+vbaCUezg5sB8TvctJiBxajNH8l7WyYyPrWXISV8k+Mad7qoTXJPBpAvc1frrv3aF8k2rXAfkXz4HEz4N615whftxX3fNMrmDXT4GTnJX+xqDkgmuOp431BXK0VbXrFA0xgWFt/4XCka5NvThMzrOp367a39f/TTkj3RNKB/+1QWXLW+7TsuTvu2uMBNVb4DcIV0Xgts/cFfUBSN6/rmqwuNXw8bX4EtPunPpiXhs700Csahrxhp7pusP2B8tNS5I7e/25qDVV4EggOssng1sw3UWf0FVVySsUwxUq2pcRH4CxFT11j3t1wJBL4l5HX+LH3B3IQyc0P5Wa/l6wo9eS27FYiooYm74Kn4bvJvHY5/kT3ya36bdRSCrgKFNK9FABrEZVxMsPhzf0992O8gbDvXezV+fvhPKV8F797oOtnjC+DZZxdBc6db/xlvw4XxXA7joPlcreOKr7g6Qb77jmgE++LOrJQya1P157anAbCsox5zsahV9RdXVXqywNQdQn401JCKfAn6Du330PlX9iYjcDixS1QVe89HPAMU1DX1TVVv3tE8LBL1gy9vw0AXuDpF356EIrx37O1ZureDUqvmMD68grsIfY5/i6sCz+IkT9mWSFvc6LrNLXCE+bAac/sOOTtC37nEdc6M/6dqrBx3pbrNr2Al3TgLUtXHnDISqDa5J5sw74KhLdr1Xu01rg7tCLRjpCvH6be5uGmPMPrNB54wTi7jOtxduheWPAVARGExjBIqpI1da2O4bwvLBF5Ix/RLqggOZlbuN3E3Pw9RL3d00zZVw1MWucN6TcJNrV267zfJZ7/7xM37srogr17i255HHJ/mkjTFggSB1bXod/nEDHH0pjJlF/E+fxddaiyKENUC6RPhT8HMERhzDpRtuRqdfgXz6zq5vnzPG9Gs21lCqKF0Mb94FJ1wHhWOIvX43Ur0R30u30xK4m6ZInH/EzuIi/2v8bch3uTr2CJdfcot7uKb2U0j+iN3vWDHGHPIsEBxK3vm9e2Bn5VOo+BFVfh89l5m+NRwbXcO/Sq5l7Fk381xtM1dMG44vcGPHtntr6jHGHLIsEBwqomH3hOlhs3gxbTa+FY9zkm85w0/7KpKXxrbV93PeRT/Al9nD4RSMMSnDAkF/puo6gN+8C16+A4BHWq/muysP59NH/oojTh7I+SO9u2xmHtuHGTXGHMwsEPRnr98Jr/3aGxLBuWvjcK49+TC+e9b4/jWipjGmz1gg6K9UYfGDaLgRDWRwzxH/x8IVW7n60yfx5U+M6evcGWP6EQsE/dW296F2M29O+C9uWDKU6pVZXH7CmVx54ui+zpkxpp+xQNBfrX0WFT+3b5zAqFED+McXpjM4P2Pv2xljTCfWiNxfNVcRSctnTZ2PK08abUHAGLPfLBD0V62NNGo6uekBzpjUxXjtxhjTQxYI+qtwE7XRdI4eWUB6YP9mLDLGGLBA0G9FQ/VUR9OYPrKwr7NijOnnLBD0Uy2N9TRpBtNHWSAwxnw8Fgj6qUhLPU1kcPSIgr7OijGmn7NA0E9JuIl4MJv8zGBfZ8UY089ZIOgvVjzpJpTxBGPNSHpO3+XHGHPIsEDQXzx6BbxxF8TjoEqGthDIyO3rXBljDgEWCPqb+lI02kqAGGlZFgiMMR9fUgOBiJwtImtEZL2IzO3i/ZEislBEPhCRpd5k92ZPqj6irrYGgIzsgr7NizHmkJC0QCAifuAe4BxgEnCpiEzqtNp/AY+o6jTgEuC3ycpPvxfMcr+r1lNeXQVAdq5NMmOM+fiSWSM4FlivqhtUNQzMBy7otI4CbaVZPrA9ifk5ODVVQkvN3tcLpLvf1RuoqqoGIDffniEwxnx8yQwEw4CtCculXlqi24DLRKQUeAb4Vlc7EpFrRWSRiCyqqKhIRl77zqNXwj//c8/rqEKo3r2u+ojaOhc4Ci0QGGN6QV93Fl8KPKCqw4FPAX8Skd3ypKrzVHWmqs4sKSk54JnsVRVroaGsY7l2CzQmLNdvh9qtu2wSbq4DjQFQvXkZr6/YDECeBQJjTC9I5nwE24ARCcvDvbREVwNnA6jqWyKSARQD5UnMV9+afymMOhHO/x+3HKqDaCsNITfdZGT+1/GH60n/6otsqmriv59dzdo1K3kjA5bGx3BUeCPj40sB8GXYcwTGmI8vmYHgPWCsiIzBBYBLgC90WmcLMBt4QEQmAhnAIdb200n99o4+gXgcQnU0Nzcy+//9m+ZwjCd1LQOlllN/uZAd9a0EfMLcGYWwAuLHXIMuvYMvxRe47dMsEBhjPr6kBQJVjYrIdcDzgB+4T1VXiMjtwCJVXQD8B3CviNyI6zi+UlU1WXnqc+FmiDRDpAWAiupKSlB2VtWSnu3j6BEFDN9US0a8mVxt5IaLpjNlWAGTWpfACjj6yKMgfhF88Ce3PwsExphekNSpKlX1GVwncGLarQmvVwInJTMPB4t/rdjJqEAN44Haujq+9+fFLFuxjNfTITcQ46lvfoKiQCv8rBmAf142lOCokW7jlbXud2YBHJkQCGyICWNML7A5iw+Ap5ZsI//xS1lGPuP9sKWskn9tK+MHMwthGRRnxJHsNKjc0r5NsG4T7g5cIFTrfmcUQMmEjh0HbHpKY8zHZ4EgSbbVtlDXHOH/Xt/I8++vZXnGh7RKBiiMyhX+/NnjOMG3ApaBRFvdRg0Jj1FUb4CWWoiG3G9wNQJ/0AWDitUgcoDPyhhzKLJAkATNL/2Cb/87j23hLD4ffI07J4yETZCuIQDyA1FOOHxAR5OP12dAw86Onbw7D175GQycBOPPAfF39AlcsxCaqw7Y+RhjDm0WCHpbayNZr/2EC+On85mC5eSEdsLmTo9GRJrc77Ymn3gE4jFo2OGWh06H7e+71+UrYeQJrjbQVgNIy3I/xhjTCywQ9JLmcJRv//UDjkyv4AbgzIyVLgj4gq6gT9RWAwjVdaRFQ1C/A9Jy4TO/h8p1sPppWP8itNZDuo0rZIxJjr5+sviQsLMuxE+fWcWLq8p5Z+lyAAZFvfb+U27efYNIsxs2oq3tHyAScjWC3MFQMh4mnguZRS493Axp2ck/EWNMSrIawce07NWnuOy5KPXxNC6ZOZKbh1W7JyfazLgSFt0HBSNg6zsd6dHQ7jWCxjIXCNoEM73nDpo6Rh81xpheZjWCnmgsh7unQ/mqXZJXvf8qU17+Ej/NeYSNGZfx0+LnKYwldOLmDIKcEvjGm3Dh73bdZ6Slo48AXCBoqoSsAR1pwUw3xlCozr02xpgksEDQEzuXQvVHsHNZe9LGyiZeeNI92HX6wAYAfK/8pKPDFzru+c8shIJRkDieXrhp16ahaAiaKyG7uCOtrRbQVGVNQ8aYpLFA0BN13lh5XlOOqnLL35dxmiwGID2S0MRT9ZEr9P1pMOjIjnR/wN0KmueNxN25RhBucmMQZSUGAu+BseZKqxEYY5LGAkFP1HuBoNXNCfDoolJWbdjEkfKRS69Y07Hu+hegcBR8aQF88qZd9/Pl5+Gsn7jXjWWwczkUj9/1GF3VCCLN1kdgjEkaCwQ94dUIWpvquPgPb3HrguXMHp4wNl7n20NzBsOoE3Yt1MGNDZSR714veRiiLa4zGaCu1P3u3EfQxpqGjDFJYoGgJ7yr9e1lZbyzsZrTJw5i7ineBDnZAzvWO+9u93v3uXU6BL0C/cOHXW1gtDfmXt0eagRgTUPGmKSx20d7wgsEVVWVDMhO4+5LpuFb7c0JUDIemsrdg2DTvwQoHDar+30lFuijToCAt1znzUrWXY0gaDUCY0xyWI1gb1Tbr9ab66s5ZXwJPp9As5tAnuJx7ndOiRsCYsaVUDi6+/0lXuXnDe/oEG7rI9ilsziz69fGGNOLLBDsTai2fWyg9HgzZ04a5NJbvEBQ4nX2ZvdwLuXEAj1vaMdQ0u19BEUJ6yYEDRtbyBiTJBYIuhKqh2fnuls6691QEXEVhqSHOXOS9+Rvc7Vr1inwJo/Zn0CQP6wjEDRVuPkG/MGu17W7howxSWKBoCuv/hLe+R18+FdXQAM7GMCwjDC+RX+E1gYXCLKKOgJATwNB4t0/ecN2nVym811Gu3QWWyAwxiRHUgOBiJwtImtEZL2IzO3i/V+LyBLvZ62I1CYzPz3W1l4fzKaszD0pHMsfib+hFJ75T1j2qGsayirqKLxzBnazs078aR2v84a6GkDbXUZZnQOB1QiMMcmXtLuGRMQP3AOcAZQC74nIAm+eYgBU9caE9b8FTEtWfvaJVwtAhC2lpQwCCoeNhXpvjoDK9a5GkFkEuUNg4GQYfmzP9p04q1hb7SCQ6fohisbsum4g8TkCCwTGmORIZo3gWGC9qm5Q1TAwH7hgD+tfCvw1ifnpuaZK97u1geoqN2tYzqDDOt6vXNtRIwiku0Hlxp6+/8eLuAnrGThx13R/wM1nAFYjMMYkTTIDwTBga8JyqZe2GxEZBYwBXu7m/WtFZJGILKqoqOj1jO6mrUYQbqSxpoJWSUcSm34q17qpIjOLut5+n3lPKZdM3P2ttgBggcAYkyQHywNllwCPqWqsqzdVdR4wD2DmzJna1Tq9JhpurxFEmuvR5hrCmfmkJ84QVrvZ/U58+GtffPpOyB+xe3rnGgG4foLWOmsaMsYkTTIDwTYgsbQb7qV15RLgm0nMS8/VbaXtCr26ppp8GlwTUNsYQYmy9rNGcMzVXafnD989ra3D2GoExpgkSWbT0HvAWBEZIyJpuMJ+QeeVRGQCUAi8lcS89Fztlo6XtTUUSiPpecUdnbyZhR3rjjyhd4+d2JHcxpqGjDFJlrQagapGReQ63MSNfuA+VV0hIrcDi1S1LShcAsxX1eQ2+fRUW8ct0NxQS7G/ibScAR1NOaf/yN0COvL43e/y2V9Xv+g6hrsSzATEdUobY0wSJLWPQFWfAZ7plHZrp+XbkpmHfRZtBSDmzyTUVMcAX5PrFC4ZD9/ZANn72S+wJyOO6f69YKa7zbSr2oIxxvSCg6Wz+OARCwOwI5pNmjaRTUNHc1AygsDeBLNswDljTFLttY9ARM4T2dMA+4eWaMTVCOrIZZDU4NPY/ncK94ZgpvUPGGOSqicF/MXAOhH5hdexe0j7aIcbVXTQ4GEMF+/Bsl57XmA/nPBNOOP2vju+MeaQt9emIVW9TETycE/+PiAiCtwP/FVVG5KdwQNte2Ut44GC4sFQ5iUm3il0oI3o4dAVxhizn3rU5KOq9cBjuGEihgCfAd73xgc6pJTXuAnqAzkJA8AVdPHwlzHGHCJ60kdwvog8AbwCBIFjVfUcYCrwH8nN3oEViyuVdV4lJ7E5aMARfZMhY4w5AHpy19BFwK9V9dXERFVtFpFuHpHtnz6qaIRYK/FgAF+GN6RERoHdtWOMOaT1JBDcBuxoWxCRTGCQqm5S1ZeSlbG+sLS0jjSi7oGxtByXmNflOHnGGHPI6EkfwaNAPGE55qUdclZsryPLF0OC6R21gLyhfZspY4xJsp4EgoA3nwAA3uu0Pazfb63YXs/ALBB/upt4BiwQGGMOeT0JBBUicn7bgohcAFQmL0t9Ix5XVm2vpzgT1zQ09gzwp8NxX+3rrBljTFL1pI/ga8BfROR/AcFNNvOlpOaqD2ytaaahNUpRhkA4zQ0o94Pyvs6WMcYkXU8eKPsIOF5EcrzlxqTnqg+s3FbHKNlJQZpCzEb6NMakjh4NOicinwYmAxnijYKpqofUuAfRlQv4d/rNxBuOgIzcvs6OMcYcMD15oOz3uPGGvoVrGpoDjEpyvg64ktIXAPDVbXF9A8YYkyJ60ll8oqp+CahR1R8BJwDjkputAyweZ0Lju+51LAyBQ/KmKGOM6VJPAkHI+90sIkOBCG68oUNGaMsiCqjvSLAagTEmhfSkj+AfIlIA/BJ4Hzez+73JzNSBVvfhP8lITLBpIY0xKWSPNQJvQpqXVLVWVR/H9Q1M6Dzd5B62P1tE1ojIehGZ2806nxeRlSKyQkQe3ucz6AXBDS+yMT6oI8FvTUPGmNSxx0CgqnHgnoTlVlWt68mORcTvbXsOMAm4VEQmdVpnLPA94CRVnQzcsE+57w2NFRTVLec5TuhIsxqBMSaF9KSP4CURuUhkn2dPPxZYr6obvGEp5gMXdFrnGuAeVa0BUNUD/wTXlrcAWFfwSWibkdNqBMaYFNKTQPBV3CBzrSJSLyINIlK/t42AYbinkNuUemmJxgHjROQNEXlbRM7uakcicq2ILBKRRRUVFT049D4Iu+fjBgwc1jHiqNUIjDEpZK+BQFVzVdWnqmmqmuct5/XS8QPAWGAWbirMe72O6c55mKeqM1V1ZklJSS8d2mlqaQFgzMACSMt2iVYjMMakkL3eNSQiJ3eV3nmimi5sAxLneBzupSUqBd5R1QiwUUTW4gLDe3vLV28pq2ngMGD04EJY4wUCqxEYY1JIT24f/U7C6wxc2/9i4LS9bPceMFZExuACwCXAFzqt8ySuJnC/iBTjmoo29CBPvaamoRmAUSV5CTUCCwTGmNTRk0HnzktcFpERwG96sF1URK4Dngf8wH2qukJEbgcWqeoC770zRWQlbsKb76hq1b6fxv5r9pqGSvJzEvoIrGnIGJM6ejToXCelwMSerKiqzwDPdEq7NeG1Ajd5P32iOeQCQVpahvURGGNSUk/6CP4H9zQxuM7lo3FPGB8SQi3eCBr+oAUCY0xK6kmNYFHC6yjwV1V9I0n5OeBaWluJ4icg0hEIrLPYGJNCehIIHgNCqhoD98SwiGSpanNys3ZghFtbiYv3MbT1EVhnsTEmhfToyWIgM2E5E3gxOdk5sMLRONFIK+oLuoT2GoE1DRljUkdPAkFG4vSU3uus5GXpwKlobCVADPV3CgRWIzDGpJCeBIImEZnetiAiM4CW5GXpwNlZFyJIFGmvEdjto8aY1NOTPoIbgEdFZDtuqsrBuKkr+73y+hBBiSFtBb/VCIwxKagnD5S9JyITgPFe0hpvSIh+b2d9iCKi+AOd+wgsEBhjUkdPJq//JpCtqstVdTmQIyLfSH7Wkq+svpU0ieELegX/4KOgeDwUjunbjBljzAHUkz6Ca1S1tm3BmzvgmqTl6AAqqw+RE4h39BEMOByuexdyeneEU2OMOZj1JBD4Eyel8WYeOyR6U8vqQ2QF1D1VbIwxKaonncXPAX8TkT94y18Fnk1elg6csvoQWX4LBMaY1NaTQHAzcC3wNW95Ke7OoX6vrL6VzNwY+DL3vrIxxhyiejJDWRx4B9iEm4vgNGBVcrOVfI2tURpbo2T44lYjMMaktG5rBCIyDjdpzKVAJfA3AFU99cBkLbnK6t2oo+kSs0BgjElpe2oaWg28BpyrqusBROTGA5KrA6AtEKRJzIadNsaktD01DX0W2AEsFJF7RWQ27sniQ0J5fSsAQaLg25/5eYwx5tDQbSBQ1SdV9RJgArAQN9TEQBH5nYiceYDylzSVjS4Q+LEagTEmtfWks7hJVR/25i4eDnyAu5Nor0TkbBFZIyLrRWRuF+9fKSIVIrLE+/nKPp/BfqpqChPwCT6NWB+BMSal7VObiPdU8TzvZ4+8B8/uAc7AzXP8nogsUNWVnVb9m6pety/56A3VjWEKs9OQWMSahowxKa0nTxbvr2OB9aq6QVXDwHzggiQeb59UN4cZkJ0GsYg1DRljUloyA8EwYGvCcqmX1tlFIrJURB4TkRFd7UhErhWRRSKyqKKiolcyV90Upqg9EFjTkDEmdSUzEPTEP4DRqnoU8ALwYFcrqeo8VZ2pqjNLSnpnQLjqJtc0RNwCgTEmtSUzEGwDEq/wh3tp7VS1SlVbvcU/AjOSmJ9dVDe1NQ2FwWeBwBiTupIZCN4DxorIGBFJAy4BFiSuICJDEhbP5wANXRGJxalriTAgyw8atz4CY0xKS9rtMqoaFZHrgOcBP3Cfqq4QkduBRaq6APi2iJwPRIFq4Mpk5SdRTXMYgOJMLw767a4hY0zqSmoJqKrPAM90Srs14fX3gO8lMw9dqWlyM20WtQUCaxoyxqSwvu4s7hNVTa2AUpwedQnWNGSMSWEpGQiqm8J8wf8yxzx2vEuwpiFjTApLyUBQ0xzhHN87HQlWIzDGpLCUDAQNoQgNZHUkWB+BMSaFpWggiNIkCYHAHigzxqSwFA0EEVr9OR0JFgiMMSksRQNBlIg/uyPBmoaMMSksJQNBYyhKRuKNQtZZbIxJYSkZCBpCUbID8Y4Eu33UGJPCUjIQ1IciZPoTA4HVCIwxqSslA0Fja5QMX0IgsD4CY0wKS8lA0BCKkpkYCOyuIWNMCku5QKCqNLZGSbdAYIwxQAoGguZwjFhcSffFOhKjrd1vYIwxh7iUCwSNrW7E0XRJCATpuX2UG2OM6XspFwgaQm4ugjSJQckE+NobUDy2j3NljDF9J+UCQX3I1QiCEoNAOgw+so9zZIwxfSvlAkFjWyAgareNGmMMSQ4EInK2iKwRkfUiMncP610kIioiM5OZH3C3jgIEiNndQsYYQxIDgYj4gXuAc4BJwKUiMqmL9XKB64F3Or+XDG19BBYIjDHGSWaN4FhgvapuUNUwMB+4oIv1fgz8NxBKYl7atdcINGJNQ8YYQ3IDwTBga8JyqZfWTkSmAyNU9Z972pGIXCsii0RkUUVFxcfKVENrFBHwadRqBMYYQx92FouID7gT+I+9rauq81R1pqrOLCkp+VjHbQhFyEkLILGIBQJjjCG5gWAbMCJhebiX1iYXOBJ4RUQ2AccDC5LdYdwQipKbEYC4NQ0ZYwwkNxC8B4wVkTEikgZcAixoe1NV61S1WFVHq+po4G3gfFVdlMQ80RiKkpMRAKsRGGMMkMRAoKpR4DrgeWAV8IiqrhCR20Xk/GQdd28aWiPkZgQtEBhjjCepU3Op6jPAM53Sbu1m3VnJzEubhlCUouw0aLSmIWOMgRR9sthqBMYY0yHlAkF9KEpOelsfgU1RaYwxKRcIGkIR8trvGrJJ640xJqUCQTgapzUaJzfdb01DxhjjSalA0DYpTW6aAGpNQ8YYQ4oFgrYB5/LSxSVY05AxxqRaIHA1gry2ioA1DRljTIoGgqC6BGsaMsaYVAsErmkoN80LBNY0ZIwxqRYIXI0gp71GYE1DxhiTUpfEbXcNZfvjLsGahoz52CKRCKWlpYRCB2RuKbMXGRkZDB8+nGCw5xe6KRUImsMxALIC1jRkTG8pLS0lNzeX0aNHIyJ9nZ2UpqpUVVVRWlrKmDFjerxdSjUNtUZdIAjiflvTkDEfXygUYsCAARYEDgIiwoABA/a5dpZSgSAUiZMW8OGLu05jaxoypndYEDh47M93kVKBoDUaIz3gg7jrK7BhqI0xJsUCQSgSJyPojTME4Lc+AmOMSalA0F4jiIVdgjUNGWP2QTQa7essJEVKXRK3RuJe05BXI7CmIWN61Y/+sYKV2+t7dZ+Thubxw/Mm73W9Cy+8kK1btxIKhbj++uu59tpree6557jllluIxWIUFxfz0ksv0djYyLe+9S0WLVqEiPDDH/6Qiy66iJycHBobGwF47LHHePrpp3nggQe48sorycjI4IMPPuCkk07ikksu4frrrycUCpGZmcn999/P+PHjicVi3HzzzTz33HP4fD6uueYaJk+ezN13382TTz4JwAsvvMBvf/tbnnjiiV79jD6upAYCETkbuAvwA39U1Z93ev9rwDeBGNAIXKuqK5OVn9ZozGsaanIJ1jRkzCHjvvvuo6ioiJaWFo455hguuOACrrnmGl599VXGjBlDdXU1AD/+8Y/Jz89n2bJlANTU1Ox136Wlpbz55pv4/X7q6+t57bXXCAQCvPjii9xyyy08/vjjzJs3j02bNrFkyRICgQDV1dUUFhbyjW98g4qKCkpKSrj//vv58pe/nNTPYX8krSQUET9wD3AGUAq8JyILOhX0D6vq7731zwfuBM5OVp5CbTWCmN01ZEwy9OTKPVnuvvvu9ivtrVu3Mm/ePE4++eT2++mLiooAePHFF5k/f377doWFhXvd95w5c/D7/QDU1dVxxRVXsG7dOkSESCTSvt+vfe1rBAKBXY53+eWX8+c//5mrrrqKt956i4ceeqiXzrj3JPOS+FhgvapuABCR+cAFQHsgUNXEOmQ2oEnMT0eNwO4aMuaQ8sorr/Diiy/y1ltvkZWVxaxZszj66KNZvXp1j/eReNtl5/vws7Oz21//4Ac/4NRTT+WJJ55g06ZNzJo1a4/7veqqqzjvvPPIyMhgzpw57YHiYJLMzuJhwNaE5VIvbRci8k0R+Qj4BfDtJOYnoUbQ1ll88H0hxph9V1dXR2FhIVlZWaxevZq3336bUCjEq6++ysaNGwHam4bOOOMM7rnnnvZt25qGBg0axKpVq4jH43tsw6+rq2PYMFeUPfDAA+3pZ5xxBn/4wx/aO5Tbjjd06FCGDh3KHXfcwVVXXdV7J92L+vyuIVW9R1UPB24G/qurdUTkWhFZJCKLKioq9vtYHX0E1jRkzKHk7LPPJhqNMnHiRObOncvxxx9PSUkJ8+bN47Of/SxTp07l4osvBuC//uu/qKmp4cgjj2Tq1KksXLgQgJ///Oece+65nHjiiQwZMqTbY333u9/le9/7HtOmTdvlLqKvfOUrjBw5kqOOOoqpU6fy8MMPt7/3xS9+kREjRjBx4sQkfQIfj6gmpzVGRE4AblPVs7zl7wGo6s+6Wd8H1Khq/p72O3PmTF20aNF+5enkXyxk+sgCfjP6bXhuLnxnA2QP2K99GWOcVatWHbQF3MHiuuuuY9q0aVx99dUH5HhdfScislhVZ3a1fjJrBO8BY0VkjIikAZcACzplbGzC4qeBdUnMT0eNoGGn6x/IKkrm4YwxhhkzZrB06VIuu+yyvs5Kt5LWSK6qURG5Dnged/vofaq6QkRuBxap6gLgOhE5HYgANcAVycoPJPQRNJZBziCw8VGMMUm2ePHivs7CXiW1t1RVnwGe6ZR2a8Lr65N5/M7aawSVOyB38IE8tDHGHLT6vLP4QFHVjhpBQ5kFAmOM8aRMIAjH3Kxk6UE/NO60QGCMMZ6UCQShiAsEWRKBlhrIsUBgjDGQQoGgbXay/Lg3rkjuoD7MjTHGHDxSJxB4NYL8WJVLyO3+gRFjzKErJyenr7Nw0EmZMRbaagR5ES8Q5FiNwJhe9+xc2Lmsd/c5eAqc8/O9r9fPRKPRg2bcoZSpEbT1EeRYIDDmkDJ37txdxg667bbbuOOOO5g9ezbTp09nypQpPPXUUz3aV2NjY7fbPfTQQ+3DR1x++eUAlJWV8ZnPfIapU6cydepU3nzzTTZt2sSRRx7Zvt2vfvUrbrvtNgBmzZrFDTfcwMyZM7nrrrv4xz/+wXHHHce0adM4/fTTKSsra8/HVVddxZQpUzjqqKN4/PHHue+++7jhhhva93vvvfdy44037u/HtitV7Vc/M2bM0P2xaFOVjrr5af3oiTtUf5in2tq0X/sxxuxq5cqVfXr8999/X08++eT25YkTJ+qWLVu0rq5OVVUrKir08MMP13g8rqqq2dnZ3e4rEol0ud3y5ct17NixWlFRoaqqVVVVqqr6+c9/Xn/961+rqmo0GtXa2lrduHGjTp48uX2fv/zlL/WHP/yhqqqecsop+vWvf739verq6vZ83XvvvXrTTTepqup3v/tdvf7663dZr6GhQQ877DANh8OqqnrCCSfo0qVLuzyPrr4T3IO8XZarB0e95ABo6yNIi3vDywYy+jA3xpjeMm3aNMrLy9m+fTsVFRUUFhYyePBgbrzxRl599VV8Ph/btm2jrKyMwYP3fLegqnLLLbfstt3LL7/MnDlzKC4uBjrmGnj55Zfb5xfw+/3k5+fvdaKbtsHvwE14c/HFF7Njxw7C4XD73AndzZlw2mmn8fTTTzNx4kQikQhTpkzZx0+raykTCEJeH0GahiCQCb6UaRUz5pA3Z84cHnvsMXbu3MnFF1/MX/7yFyoqKli8eDHBYJDRo0fvNsdAV/Z3u0SBQIB4PN6+vKe5Db71rW9x0003cf755/PKK6+0NyF15ytf+Qo//elPmTBhQq8OaZ0ypWFbjSAYD0FaVh/nxhjTmy6++GLmz5/PY489xpw5c6irq2PgwIEEg0EWLlzI5s2be7Sf7rY77bTTePTRR6mqcn2MbXMNzJ49m9/97ncAxGIx6urqGDRoEOXl5VRVVdHa2srTTz+9x+O1zW3w4IMPtqd3N2fCcccdx9atW3n44Ye59NJLe/rx7FXKBIK2GkEgFoKgBQJjDiWTJ0+moaGBYcOGMWTIEL74xS+yaNEipkyZwkMPPcSECRN6tJ/utps8eTLf//73OeWUU5g6dSo33XQTAHfddRcLFy5kypQpzJgxg5UrVxIMBrn11ls59thjOeOMM/Z47Ntuu405c+YwY8aM9mYn6H7OBIDPf/7znHTSST2aYrOnkjYfQbLs73wE89/dwty/L2PVlL+SWbMGrnsvCbkzJvXYfAQH1rnnnsuNN97I7Nmzu13nYJqP4KASirgagd9qBMaYfqi2tpZx48aRmZm5xyCwP1Kms7g16voI/NEWCwTGpLhly5a1PwvQJj09nXfeeaePcrR3BQUFrF27Nin7TplA8OmjhjBhSB6+V/4fZBb0dXaMOaSoKtKPJnqaMmUKS5Ys6etsJMX+NPenTNPQ8MIsThlXgkSa7a4hY3pRRkYGVVVV+1UAmd6lqlRVVZGRsW/PSaVMjaBdpNmahozpRcOHD6e0tJSKioq+zorBBebhw4fv0zYpGAhaIJjZ17kw5pARDAbbn4g1/VNSm4ZE5GwRWSMi60Vkbhfv3yQiK0VkqYi8JCKjkpkfwKsRZO99PWOMSRFJCwQi4gfuAc4BJgGXisikTqt9AMxU1aOAx4BfJCs/AKh6gcBqBMYY0yaZNYJjgfWqukFVw8B84ILEFVR1oao2e4tvA/vWsLWvYmHQuAUCY4xJkMw+gmHA1oTlUuC4Pax/NfBsV2+IyLXAtd5io4is2c88FQOV/Og7wHf2cxcHDXcuhwY7l4OTncvBaX/Ppdum94Ois1hELgNmAqd09b6qzgPm9cJxFnX3iHV/Y+dycLJzOTjZuexZMgPBNmBEwvJwL20XInI68H3gFFVtTWJ+jDHGdCGZfQTvAWNFZIyIpAGXAAsSVxCRacAfgPNVtTyJeTHGGNONpAUCVY0C1wHPA6uAR1R1hYjcLiLne6v9EsgBHhWRJSKyoJvd9ZaP3bx0ELFzOTjZuRyc7Fz2oN8NQ22MMaZ3pcxYQ8YYY7pmgcAYY1JcygSCvQ13cbATkU0isszrS1nkpRWJyAsiss773Xtz1/UiEblPRMpFZHlCWpd5F+du73taKiLT+y7nu+vmXG4TkW3ed7NERD6V8N73vHNZIyJn9U2udyciI0RkoTfEywoRud5L73ffyx7OpT9+Lxki8q6IfOidy4+89DEi8o6X5795N+AgIune8nrv/dH7dWBVPeR/AD/wEXAYkAZ8CEzq63zt4zlsAoo7pf0CmOu9ngv8d1/ns5u8nwxMB5bvLe/Ap3APFgpwPPBOX+e/B+dyG/CfXaw7yftbSwfGeH+D/r4+By9vQ4Dp3utcYK2X3373vezhXPrj9yJAjvc6CLzjfd6PAJd46b8Hvu69/gbwe+/1JcDf9ue4qVIj2OtwF/3UBcCD3usHgQv7LivdU9VXgepOyd3l/QLgIXXeBgpEZMgByWgPdHMu3bkAmK+qraq6EViP+1vsc6q6Q1Xf91434O7sG0Y//F72cC7dOZi/F1XVRm8x6P0ocBpuPDbY/Xtp+74eA2bLfswQlCqBoKvhLvb0h3IwUuBfIrLYG3IDYJCq7vBe7wQG9U3W9kt3ee+v39V1XpPJfQlNdP3iXLzmhGm4q89+/b10Ohfoh9+LiPhFZAlQDryAq7HUqrslH3bNb/u5eO/XAQP29ZipEggOBZ9Q1em40Vy/KSInJ76prm7YL+8F7s959/wOOBw4GtgB/L8+zc0+EJEc4HHgBlWtT3yvv30vXZxLv/xeVDWmqkfjRmM4FpiQ7GOmSiDo0XAXBzNV3eb9LgeewP2BlLVVz73f/enp7O7y3u++K1Ut8/5548C9dDQzHNTnIiJBXMH5F1X9u5fcL7+Xrs6lv34vbVS1FlgInIBrimsbEigxv+3n4r2fD1Tt67FSJRDsdbiLg5mIZItIbttr4ExgOe4crvBWuwJ4qm9yuF+6y/sC4EveXSrHA3UJTRUHpU5t5Z/BfTfgzuUS786OMcBY4N0Dnb+ueO3I/wesUtU7E97qd99Ld+fST7+XEhEp8F5nAmfg+jwWAp/zVuv8vbR9X58DXvZqcvumr3vJD9QP7q6Htbj2tu/3dX72Me+H4e5y+BBY0ZZ/XFvgS8A64EWgqK/z2k3+/4qrmkdw7ZtXd5d33F0T93jf0zLcxEV9fg57OZc/eXld6v1jDklY//veuawBzunr/Cfk6xO4Zp+lwBLv51P98XvZw7n0x+/lKNyEXUtxgetWL/0wXLBaDzwKpHvpGd7yeu/9w/bnuDbEhDHGpLhUaRoyxhjTDQsExhiT4iwQGGNMirNAYIwxKc4CgTHGpDgLBMZ0IiKxhBErl0gvjlYrIqMTRy415mCQzMnrjemvWtQ94m9MSrAagTE9JG5OiF+ImxfiXRE5wksfLSIve4ObvSQiI730QSLyhDe2/IcicqK3K7+I3OuNN/8v7wlSY/qMBQJjdpfZqWno4oT36lR1CvC/wG+8tP8BHlTVo4C/AHd76XcD/1bVqbg5DFZ46WOBe1R1MlALXJTUszFmL+zJYmM6EZFGVc3pIn0TcJqqbvAGOdupqgNEpBI3fEHES9+hqsUiUgEMV9XWhH2MBl5Q1bHe8s1AUFXvOACnZkyXrEZgzL7Rbl7vi9aE1zGsr870MQsExuybixN+v+W9fhM3oi3AF4HXvNcvAV+H9slG8g9UJo3ZF3YlYszuMr0Zoto8p6ptt5AWishS3FX9pV7at4D7ReQ7QAVwlZd+PTBPRK7GXfl/HTdyqTEHFesjMKaHvD6Cmapa2dd5MaY3WdOQMcakOKsRGGNMirMagTHGpDgLBMYYk+IsEBhjTIqzQGCMMSnOAoExxqS4/w+r5+ZYDOBSWQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.2, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Jamie-0.1.h5')\n",
    "model.save_weights(\"Jamie-0.1.weights.h5\")\n",
    "\n",
    "results = model.predict(test_images)\n",
    "##tf.compat.v1.metrics.mean_per_class_accuracy(test_labels,results,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Test\",'a')\n",
    "f.write(\"Hello, World!\")\n",
    "f.close()"
   ]
  }
 ]
}